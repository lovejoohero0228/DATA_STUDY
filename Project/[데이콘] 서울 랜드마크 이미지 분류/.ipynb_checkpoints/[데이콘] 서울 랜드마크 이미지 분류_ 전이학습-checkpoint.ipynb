{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e8d2bc5",
   "metadata": {},
   "source": [
    "# 데이콘 Basic Summer\n",
    "## 서울 랜드마크 이미지 분류 경진대회\n",
    "\n",
    "## Baseline 2 -<span style=\"color:red\"> 전이학습 (transfer learning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d85fe1ad",
   "metadata": {},
   "source": [
    "\n",
    "# I. 데이터 살펴보기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d240209b",
   "metadata": {},
   "source": [
    "## 1. 데이터 준비\n",
    "### 1.1 csv 데이터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "656b1b27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_name</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>001.PNG</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>002.PNG</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>003.PNG</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>004.PNG</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>005.PNG</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  file_name  label\n",
       "0   001.PNG      9\n",
       "1   002.PNG      4\n",
       "2   003.PNG      1\n",
       "3   004.PNG      1\n",
       "4   005.PNG      6"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pandas 사용하여 데이터 불러오기\n",
    "\n",
    "import pandas as pd\n",
    "label_df = pd.read_csv('dataset/train.csv')\n",
    "label_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4463066",
   "metadata": {},
   "source": [
    "### 1.2 이미지 데이터\n",
    "\n",
    "데이터 이미지의 local address와 label 값을 list에 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "18417f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "# CNN 모델에서 꼭 필요한 두가지 모듈\n",
    "# 'glob' : 파일의 경로명을 리스트로 뽑을 때 사용\n",
    "\n",
    "def get_train_data(data_dir):\n",
    "    img_path_list = []\n",
    "    label_list = []\n",
    "    \n",
    "    # get image path\n",
    "    img_path_list.extend(glob(os.path.join(data_dir,'*.PNG')))\n",
    "    img_path_list = list(map(lambda x: x.replace('\\\\','/', 10), img_path_list))\n",
    "    img_path_list.sort(key=lambda x:int(x.split('/')[-1].split('.')[0]))\n",
    "\n",
    "    # get label\n",
    "    label_list.extend(label_df['label'])\n",
    "    \n",
    "    #print(img_path_list)\n",
    "    #print(label_list)\n",
    "    \n",
    "    return img_path_list,label_list\n",
    "\n",
    "def get_test_data(data_dir):\n",
    "    img_path_list = []\n",
    "    \n",
    "    # get image path\n",
    "    img_path_list.extend(glob(os.path.join(data_dir,'*.PNG')))\n",
    "    img_path_list = list(map(lambda x: x.replace('\\\\','/', 10), img_path_list))\n",
    "    img_path_list.sort(key=lambda x: int(x.split('/')[-1].split('.')[0]))\n",
    "    #print(img_path_list)\n",
    "    \n",
    "    return img_path_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c052bc5",
   "metadata": {},
   "source": [
    "## 2. 데이터 확인\n",
    "### 2.1. csv 데이터\n",
    "\n",
    "pandas의 `info()` 메소드 활용하여 데이터의 특성 살펴보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "339c9099",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 723 entries, 0 to 722\n",
      "Data columns (total 2 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   file_name  723 non-null    object\n",
      " 1   label      723 non-null    int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 11.4+ KB\n"
     ]
    }
   ],
   "source": [
    "label_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "280e7c62",
   "metadata": {},
   "source": [
    "### 2.2. 이미지 데이터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "f6f29d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_img_path, all_label = get_train_data('dataset/train')\n",
    "test_img_path = get_test_data('dataset/test')\n",
    "\n",
    "#['dataset/train/001.PNG',\n",
    "#['dataset/train\\\\001.PNG',"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "2f107609",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[9, 4, 1, 1, 6]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_label[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "72c72c6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dataset/train/001.PNG',\n",
       " 'dataset/train/002.PNG',\n",
       " 'dataset/train/003.PNG',\n",
       " 'dataset/train/004.PNG',\n",
       " 'dataset/train/005.PNG']"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_img_path[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "fb07a324",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dataset/test/001.PNG',\n",
       " 'dataset/test/002.PNG',\n",
       " 'dataset/test/003.PNG',\n",
       " 'dataset/test/004.PNG',\n",
       " 'dataset/test/005.PNG']"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_img_path[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b2aa759",
   "metadata": {},
   "source": [
    "## 3. 환경설정\n",
    "데이터를 전처리 하기위함 GPU 딥러닝 환경설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "049d8513",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "os.environ[\"CUDA_DEVISE_ORDER\"] = \"PCI_BUS_ID\"  # rrange GPU devices starting from 0\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\"  # Set the GPU 2 to use, 멀티 GPU\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "02c2c738",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "6a091865",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU availavle, using the CPU instead.\n"
     ]
    }
   ],
   "source": [
    "# GPU 체크 및 할당\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    #device = torch.device(\"cuda:0\")\n",
    "    print(\"Device:\",device)\n",
    "    print(\"There are %d GPU(s) available\"%torch.cuda.device_count())\n",
    "    print(\"We will use the GPU:\",torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"No GPU availavle, using the CPU instead.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d72885be",
   "metadata": {},
   "source": [
    "# 똥컴이라 안됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "0be6278b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.12.1\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(torch.__version__)\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "feac0c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 하이퍼 파라미터 튜닝\n",
    "CFG = {\n",
    "    'IMG_SIZE':128, # 이미지 사이즈\n",
    "    'EPOCHS':50, # 에포크 - 전체 데이터를 사용하여 학습하는 횟수\n",
    "    'LEARNING_RATE':2e-2, # 학습률\n",
    "    'BATCH_SIZE':12, #배치사이즈\n",
    "    'SEED':41, #시드\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "3265226a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델의 재현성을 위해 random seed 고정\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    \n",
    "seed_everything(CFG['SEED'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67a42912",
   "metadata": {},
   "source": [
    "# II. 데이터 전처리\n",
    "## 2. CustomDataset\n",
    "CustomDataset을 만들어 전체 dataset을 구성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "8e5be548",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.datasets as datasets  # 이미지 데이터셋 집합체\n",
    "import torchvision.transforms as transforms  # 이미지 변환 툴 - 텐서 변환, 이미지 정규화...\n",
    "\n",
    "from torch.utils.data import DataLoader  # 학습 및 배치로 모델에 넣어주기 위한 툴\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "import cv2   # 이미지 출력하고, 저장하고, 새 윈도우에 보여주고 등등\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    #필요한 변수 선언\n",
    "    def __init__(self,img_path_list,label_list,train_mode = True, transforms = None):\n",
    "        self.transforms = transforms\n",
    "        self.train_mode = train_mode\n",
    "        self.img_path_list = img_path_list\n",
    "        self.label_list = label_list\n",
    "    \n",
    "    # index번째 data를 return\n",
    "    def __getitem__(self,index):\n",
    "        img_path = self.img_path_list[index]  # 이미지 파일 경로\n",
    "        print(img_path)\n",
    "        \n",
    "        # Get image data\n",
    "        image = cv2.imread(img_path)  # 이미지 파일\n",
    "        if self.transforms is not None:   # transfrom 형태가 주어지면 transfrom하여 저장\n",
    "            image = self.transforms(image)\n",
    "            \n",
    "        if self.train_mode:\n",
    "            label = self.label_list[index]\n",
    "            return image, label\n",
    "        else:\n",
    "            return image\n",
    "        \n",
    "    def __len__(self):  # 길이 출력\n",
    "        return len(self.img_path_list)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "f0118ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 임시 데이터셋 만들어 정상 작동하는지 확인\n",
    "tempdataset = CustomDataset(all_img_path, all_label,train_mode = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "5ad03f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#plt.imshow(tempdataset.__getitem__(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28ed4df6",
   "metadata": {},
   "source": [
    "# Kernal Died ?? Prolly GPU 사용 못해서 메모리 사용량 과다로 죽은듯"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e369a788",
   "metadata": {},
   "source": [
    "## 3. Train / Validation Split\n",
    "\n",
    "학습시킬 데이터셋과 검증할 데이터셋 분리!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "0379b35b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train : Validation = 0.75: 0.25 Split\n",
    "\n",
    "train_len = int(len(all_img_path)*0.75)\n",
    "Vali_len = int(len(all_img_path)*0.25)\n",
    "\n",
    "train_img_path = all_img_path[:train_len]\n",
    "train_label = all_label[:train_len]\n",
    "\n",
    "vali_img_path = all_img_path[train_len:]\n",
    "vali_label = all_label[train_len:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "e9d0e823",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train set 길이: 542\n",
      "validation set 길이: 180\n"
     ]
    }
   ],
   "source": [
    "print(\"train set 길이:\", train_len)\n",
    "print(\"validation set 길이:\", Vali_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79f59075",
   "metadata": {},
   "source": [
    "## 4. Transfrom\n",
    "\n",
    "나뉜 데이터셋에서 이미지를 분석하기 위해 이미지 변형(transform) 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "2ecb8c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),  # Numpy 배열에서 PIL 이미지로 변형\n",
    "    transforms.Resize([CFG['IMG_SIZE'],CFG['IMG_SIZE']]), # 이미지 사이즈 변형\n",
    "    transforms.ToTensor(), #이미지 데이터를 tensor\n",
    "    transforms.Normalize(mean= (0.5,0.5,0.5),std = (0.5,0.5,0.5)) # 이미지 정규화\n",
    "])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize([CFG['IMG_SIZE'], CFG['IMG_SIZE']]),\\\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean =(0.5,0.5,0.5),std = (0.5,0.5,0.5))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25f3bcda",
   "metadata": {},
   "source": [
    "## 5. Dataloader\n",
    "- `Dataloader` Class 생성: batch 기반 딥러닝모델 학습을 위해 mini batch를 만들어주는 역할\n",
    "\n",
    "- dataset의 전체 데이터가 batch size로 나뉨\n",
    "\n",
    "- 만들었던 dataset을 input으로 넣어주면 여러 옵션(데이터 묶기, 섞기, 알아서 병렬처리)를 통해 batch 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "429d00c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Dataloader\n",
    "\n",
    "# CustomDataset class를 통하여 train dataset 생성\n",
    "train_dataset = CustomDataset(train_img_path, \n",
    "                              train_label, \n",
    "                              train_mode = True, \n",
    "                              transforms = train_transform)\n",
    "# 만든 train dataset를 DataLoader에 넣어 batch 만들기\n",
    "train_loader = DataLoader(train_dataset,\n",
    "                          batch_size = CFG['BATCH_SIZE'],\n",
    "                          shuffle=True, \n",
    "                          num_workers =0)\n",
    "\n",
    "\n",
    "# Validation 에서도 적용\n",
    "vali_dataset = CustomDataset(vali_img_path, \n",
    "                             vali_label, \n",
    "                             train_mode = True, \n",
    "                             transforms = test_transform)\n",
    "vali_loader = DataLoader(vali_dataset,\n",
    "                         batch_size = CFG['BATCH_SIZE'],\n",
    "                         shuffle=True, \n",
    "                         num_workers =0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "00ec68ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total train imgs:  542 / total train batches(iterations) : 46\n",
      "total valid imgs:  180 / total valid batches(iterations) : 16\n"
     ]
    }
   ],
   "source": [
    "train_batches = len(train_loader)\n",
    "vali_batches = len(vali_loader)\n",
    "\n",
    "print('total train imgs: ',train_len,'/ total train batches(iterations) :',train_batches)\n",
    "print('total valid imgs: ',Vali_len,'/ total valid batches(iterations) :',vali_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "5591219d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch.utils.data.dataloader.DataLoader object at 0x000002BD03F3D0D0>\n"
     ]
    }
   ],
   "source": [
    "print(vali_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac01a753",
   "metadata": {},
   "source": [
    "**train 데이터의 경우, 542 장의 이미지를, 12개짜리 배치로 나누어 1번의 epoch를 돌기 위해 46개의 batch를 반복 학습해야한다.**\n",
    "\n",
    "- 배치 사이즈 : 12 \n",
    "- train - 46 묶음 / validation - 16 묶음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32b0360a",
   "metadata": {},
   "source": [
    "# III. 모델링\n",
    "## 1. 모델 구조 정의 - <span style=\"color:red\"> 전이학습 (Transfer Learning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fe7a3be",
   "metadata": {},
   "source": [
    "Pytorch의 model 메소드를 사용하면 손쉽게 외부 모델을 불러올 수 있다.\n",
    "\n",
    "이번 베이스라인에서는 **`efficientnet_b3`** 전이학습 모델을 사용해보자.\n",
    "\n",
    "사전 학습 모델을 사용하는 것은 부정행위에 해당하니, **`weights`** 파라미터를 False로 설정하여 미리 학습된 weigt들은 가져오지 않게 하고, 랜덤하게 weight를 부여한 후 전이학습 모델 구조만 가지고 학습을 진행한다.\n",
    "\n",
    "(Pre-trained 파라미터를 True로 설정하면, ImageNet이라는 데이터셋을 대상으로 학습된 모델이 load된다.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5c2fb7a",
   "metadata": {},
   "source": [
    "### 전이학습이란?\n",
    "\n",
    "위키백과에서는 '한 분야의 문제를 해결하기 위해서 얻은 지식과 정보를 다른 문제를 푸는데 사용하는 방식'으로 정의하고 있다.\n",
    "\n",
    "이를 딥러닝 분야에서는 '이미지 분류' 문제를 해결하는데 사용했던 네트워크(DNN; Deep Neural Network)를 다른 데이터셋 혹은 다른 문제에 적용시켜 푸는 것을 의미한다. 특히나 기계의 시각적 이해를 목표로 하는 컴퓨터 비전 영역에서 전이 학습으로 수행된 모델들이 높은 성능을 보이고 있어 가장 많이 사용되는 방법 중에 하나이다. 전이학습을 수행하지 않은 모델들보다 비교적 빠르고 정확한 정확도를 달성할 수 있는 것이다. 어떻게 이런 것들이 가능할까?\n",
    "\n",
    "\n",
    "바로 네트워크가 **다양한 이미지의 보편적인 특징 혹은 피처들을 학습**했기 때문이다. 일반적으로 네트워크가 깊어질수록 서로 다른 종류의 피처들을 학습한다고 알려져 있다. 낮은 층에서 학습되는 피처를 low-level features, 깊은 층에서 학습되는 피처들을 high-level feature라고 부른다. Low-level feature의 예로는 이미지의 색이나 경계(edge) 등을 말할 수 있고, high-level feature는 이보다 더 심화된 객체의 패턴이나 형태를 의미한다. 그림 1을 보면 각의 단계에서 이미지의 특징들을 추출하는 필터를 시각화한 것이다. 단계별로 서로 다른 형태를 띄고 있음을 알 수 있다. Low-level feature의 핑터의 경우, 색의 변화나 경계의 방향등을 추출한다고 유추할 수 있고 더 올라가서 high-level feature는 동그라미가 반복되는 패턴이나 새의 부리 등의 이미지를 분류하는 데 있어서 주요한 특징들을 학습한다.\n",
    "\n",
    "![multi-level feature](https://blog.kakaocdn.net/dn/4oOip/btrs71YpILX/bJvSWZeY4KL2JhJ8y72Mrk/img.png)\n",
    "그림1. multi-level features\n",
    "\n",
    "\n",
    "네트워크가 이러한 특징을 학습하기 위해서는 대량의 데이터셋이 필요한데 가장 대표적으로 ImageNet을 들 수 있다. ImageNet은 2010년~2017년까지 매해 열린 대회 ImageNet Large-Scale Visual Recognition(ILSVRC)을 위한 데이터셋으로 자동차나 고양이를 포함한 1000개의 클래스, 총 1400만개의 이미지로 구성되어 있다. 해당 데이터셋을 가지고 이미지 분류를 수행한 모델들은 매년 뛰어난 성능을 보여주고 있는데, 2015년 이후에는 **사람보다 뛰어난 정확도를 가진 모델이 등장**하였다. 참고로 Russakovsky et al에 따르면 사람의 이미지 분류 오류는 5.1%에 달한다.\n",
    "\n",
    "\n",
    "![ILSVRC](https://blog.kakaocdn.net/dn/qhcGb/btrtb3AWeuX/7iXeYbq8gjhmlJ1Zviron0/img.png)\n",
    "그림2. ILSVRC\n",
    "\n",
    "\n",
    "2012년 AlexNet 이후에 VGG, GoogleNet, ResNet 등 주요 CNN 구조들은 전이 학습을 수행하는데 네트워크의 기저(base 또는 backbone)으로 사용되고 있다.논문을 살펴보면 ResNet을 기반으로 한 여러 모델을 찾아볼 수 있다. ImageNet으로 학습시킨 CNN 구조는 정말 많이 사용되기 떄문에 pytorch나 tensorflow 같은 딥러닝 프레임워크에 API가 저장되어 있어 간편하게 불러와서 사용할 수 있다. 이렇게 구현할 수 있는 모델의 목록은 공식문서([Pytorch](https://pytorch.org/)) 등을 통해 확인할 수 있다. \n",
    "\n",
    "주요 CNN 구조: GoogleNet , "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "6d316038",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import models\n",
    "from torchvision.models import efficientnet_b3 as efficientnet\n",
    "\n",
    "model = models.efficientnet_b3(weights=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "b485f0fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Dropout(p=0.3, inplace=True)\n",
       "  (1): Linear(in_features=1536, out_features=1000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bf5acbc",
   "metadata": {},
   "source": [
    "모델에 데이터를 학습하기 위해서는 모델의 마지막 layer의 output size와 분류할 라벨의 수를 입력해야 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "0a821151",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torch.nn import CrossEntropyLoss\n",
    "import torch.optim as optim\n",
    "\n",
    "model.fc = nn.Linear(1000, 10)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0891258",
   "metadata": {},
   "source": [
    "## 2. 모델 학습\n",
    "\n",
    "모델 학습을 하기위해 매개변수 정의하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "22f7b357",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim # 최적화 알고리즘들을 포함하는 패키지\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(params = model.parameters(), lr = CFG[\"LEARNING_RATE\"])\n",
    "scheduler = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b0fe9b9",
   "metadata": {},
   "source": [
    "### Classification model\n",
    "\n",
    "이번 베이스라인 프로젝트 모델에서는 기본 **`CNN classification 모델`** 사용했다.\n",
    "\n",
    "### Loss function\n",
    "\n",
    "손실함수로는 classification 문제이기 때문에 **`CrossEntropyLoss`**를 사용했다.\n",
    "\n",
    "손실함수는 실제 값과 모델이 예측한 값의 거리를 출력하는 함수로, 예측이 얼마나 틀렸는지 알려주는 함수이다.\n",
    "\n",
    "\"모델의 예측이 얼마나 틀렸는지\"를 어떻게 정의하느냐에 따라 어떤 Loss function을 사용할 지가 정해진다. \n",
    "\n",
    "\n",
    "### Optimizer\n",
    "\n",
    "최적화 함수로는 확률적 경사 하강법 **`SGD(Stochastic Gradient Descent)`**를 사용했다.\n",
    "\n",
    "Optimizer는 학습 데이터(Train data)를 이용하여 모델을 학습할 때 데이터의 실제 결과와 오델이 예측한 결과를 기반으로 오차를 잘 줄일 수 있게 만들어주는 역할을 한다.\n",
    "\n",
    "여기서 Learning rate, 학습률은 얼마나 빠른 속도로 이동하는 지를 결정한다.\n",
    "\n",
    "Learning rate를 엄청 크게 설정한다면 원하는 값까지 빠르게 도달할 수 있지만, 자칫하면 오히려 최소값에 수렴하지 못할 수도 있다.\n",
    "\n",
    "반면 너무 작은 경우에는 최소값에 도달할 수 있을 지는 몰라도, 시간이 매우 오래 걸리고, overfitting의 문제가 발생할 수 있다.\n",
    "\n",
    "따라서 적절한 learning rate를 설정하는 과정이 중요하다.\n",
    "\n",
    "이제 train 함수를 만들고 train 데이터를 학습시켜 validation으로 평가하는 메소드를 작성해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "712f4de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def train(model, optimizer, train_loader, scheduler, device): \n",
    "    model.to(device)\n",
    "    n = len(train_loader)\n",
    "    best_acc = 0\n",
    "    \n",
    "    for epoch in range(1,CFG[\"EPOCHS\"]+1): #에포크 설정\n",
    "        model.train() #모델 학습\n",
    "        running_loss = 0.0\n",
    "            \n",
    "        for img, label in tqdm(iter(train_loader)):\n",
    "            img, label = img.to(device), label.to(device) #배치 데이터\n",
    "            optimizer.zero_grad() #배치마다 optimizer 초기화\n",
    "        \n",
    "            # Data -> Model -> Output\n",
    "            logit = model(img) #예측값 산출\n",
    "            loss = criterion(logit, label) #손실함수 계산\n",
    "            \n",
    "            # 역전파\n",
    "            loss.backward() #손실함수 기준 역전파 \n",
    "            optimizer.step() #가중치 최적화\n",
    "            running_loss += loss.item()\n",
    "              \n",
    "        print('[%d] Train loss: %.10f' %(epoch, running_loss / len(train_loader)))\n",
    "        \n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "            \n",
    "        #Validation set 평가\n",
    "        model.eval() #evaluation 과정에서 사용하지 않아야 하는 layer들을 알아서 off 시키도록 하는 함수\n",
    "        vali_loss = 0.0\n",
    "        correct = 0\n",
    "        with torch.no_grad(): #파라미터 업데이트 안하기 때문에 no_grad 사용\n",
    "            for img, label in tqdm(iter(vali_loader)):\n",
    "                img, label = img.to(device), label.to(device)\n",
    "\n",
    "                logit = model(img)\n",
    "                vali_loss += criterion(logit, label)\n",
    "                pred = logit.argmax(dim=1, keepdim=True)  #11개의 class중 가장 값이 높은 것을 예측 label로 추출\n",
    "                correct += pred.eq(label.view_as(pred)).sum().item() #예측값과 실제값이 맞으면 1 아니면 0으로 합산\n",
    "        vali_acc = 100 * correct / len(vali_loader.dataset)\n",
    "        print('Vail set: Loss: {:.4f}, Accuracy: {}/{} ( {:.0f}%)\\n'.format(vali_loss / len(vali_loader), correct, len(vali_loader.dataset), 100 * correct / len(vali_loader.dataset)))\n",
    "        \n",
    "        #베스트 모델 저장\n",
    "        if best_acc < vali_acc:\n",
    "            best_acc = vali_acc\n",
    "            torch.save(model.state_dict(), 'data/save_data/saved/best_model.pth') #이 디렉토리에 best_model.pth을 저장\n",
    "            print('Model Saved.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "9aecebfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for img,label in  tqdm(iter(train_loader)):\n",
    "    #print(\"************\",i,\"************\")\n",
    "    #print(\"img:\",img)\n",
    "    #print(\"label:\",label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c5a0eda",
   "metadata": {},
   "source": [
    "### Logit \n",
    "\n",
    "확률화되지 않은 예측 결과를 logit이라고 한다. 확률값을 계산하지 않고 layer를 거쳐 나온 산출물을 그대로 다음 layer에 넘길 때 사용한다.\n",
    "\n",
    "즉, logit은 신경망의 최종 레이어가 내놀은 확률 값이 아닌 중간 결과물이고, 결과값의 범위가 실수 전체이다. \n",
    "\n",
    "![logit](https://velog.velcdn.com/images%2Fguide333%2Fpost%2Feb996ddf-1ff4-48e7-b8f9-fa23b622505a%2FScreenshot%20from%202021-05-17%2011-01-37.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edb1647d",
   "metadata": {},
   "source": [
    "Classification 문제이기 때문에 평가지표로는 Accuracy를 사용하여 모델의 정확도를 산출하였다.\n",
    "\n",
    "\n",
    "### Epoch\n",
    "\n",
    "딥러닝에서 epoch는 전체 트레이닝 셋이 신경망을 통과한 횟수이다.\n",
    "\n",
    "1-epoch는 전체 트레이닝 셋이 하나의 신경망에 적용되어 순전파와 역전파를 통해 한번 통과했다는 뜻이다.\n",
    "\n",
    "Epoch가 많다고 꼭 학습이 잘 되는 것은 아니다. \n",
    "Epoch가 너무 적을 경우 학습이 덜 이루어질 수 있고, 너무 많을 경우 과적합이 되는 경우도 있다. 따라서, 적절한 epoch를 설정하는 게 중요하다.\n",
    "\n",
    "이때 validation loss와 accuracy는 epoch를 언제 중단 할 지 모니터링하는 용도로 사용되기도 한다.\n",
    "\n",
    "\n",
    "### Batch size\n",
    "\n",
    "Batch size란 CPU 또는 GPU 연산 시, 하드웨어로 한번에 로드되는 데이터의 개수이다. \n",
    "\n",
    "개인 컴퓨터 환경(메모리 용량)에 따라 batch size를 조절해야하며, 이는 모델 학습 과정에 영향을 끼치기도 하기 때문에, 적절히 설정하는 것이 중요하다.\n",
    "\n",
    "\n",
    "### Backpropagation (역전파)\n",
    "\n",
    "오차 역전파법이라고도 하며 예측값과 실제값의 차이인 오차를 계산하고고, 이것을 다시 역으로 input 단에 보내, 오차가 작아지는 방향으로 가중치를 수정하는 과정을 일정 횟수 반복하는 방법이다.\n",
    "\n",
    "이때 역전파 과정에서는 앞서 언급했던 `최적화 함수`를 이용한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "1c297eac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                           | 0/46 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset/train/298.PNG\n",
      "dataset/train/085.PNG\n",
      "dataset/train/530.PNG\n",
      "dataset/train/165.PNG\n",
      "dataset/train/533.PNG\n",
      "dataset/train/535.PNG\n",
      "dataset/train/404.PNG\n",
      "dataset/train/491.PNG\n",
      "dataset/train/494.PNG\n",
      "dataset/train/360.PNG\n",
      "dataset/train/381.PNG\n",
      "dataset/train/328.PNG\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  2%|█▊                                                                                 | 1/46 [00:03<02:44,  3.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset/train/211.PNG\n",
      "dataset/train/072.PNG\n",
      "dataset/train/054.PNG\n",
      "dataset/train/143.PNG\n",
      "dataset/train/137.PNG\n",
      "dataset/train/525.PNG\n",
      "dataset/train/028.PNG\n",
      "dataset/train/123.PNG\n",
      "dataset/train/154.PNG\n",
      "dataset/train/333.PNG\n",
      "dataset/train/002.PNG\n",
      "dataset/train/430.PNG\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  4%|███▌                                                                               | 2/46 [00:07<02:39,  3.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset/train/127.PNG\n",
      "dataset/train/371.PNG\n",
      "dataset/train/513.PNG\n",
      "dataset/train/077.PNG\n",
      "dataset/train/521.PNG\n",
      "dataset/train/503.PNG\n",
      "dataset/train/338.PNG\n",
      "dataset/train/231.PNG\n",
      "dataset/train/294.PNG\n",
      "dataset/train/386.PNG\n",
      "dataset/train/538.PNG\n",
      "dataset/train/023.PNG\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  7%|█████▍                                                                             | 3/46 [00:10<02:31,  3.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset/train/253.PNG\n",
      "dataset/train/001.PNG\n",
      "dataset/train/024.PNG\n",
      "dataset/train/170.PNG\n",
      "dataset/train/498.PNG\n",
      "dataset/train/158.PNG\n",
      "dataset/train/444.PNG\n",
      "dataset/train/495.PNG\n",
      "dataset/train/411.PNG\n",
      "dataset/train/074.PNG\n",
      "dataset/train/088.PNG\n",
      "dataset/train/221.PNG\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  9%|███████▏                                                                           | 4/46 [00:13<02:21,  3.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset/train/455.PNG\n",
      "dataset/train/388.PNG\n",
      "dataset/train/226.PNG\n",
      "dataset/train/461.PNG\n",
      "dataset/train/249.PNG\n",
      "dataset/train/053.PNG\n",
      "dataset/train/197.PNG\n",
      "dataset/train/365.PNG\n",
      "dataset/train/120.PNG\n",
      "dataset/train/291.PNG\n",
      "dataset/train/113.PNG\n",
      "dataset/train/427.PNG\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 11%|█████████                                                                          | 5/46 [00:16<02:13,  3.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset/train/517.PNG\n",
      "dataset/train/192.PNG\n",
      "dataset/train/315.PNG\n",
      "dataset/train/516.PNG\n",
      "dataset/train/414.PNG\n",
      "dataset/train/118.PNG\n",
      "dataset/train/357.PNG\n",
      "dataset/train/527.PNG\n",
      "dataset/train/100.PNG\n",
      "dataset/train/487.PNG\n",
      "dataset/train/285.PNG\n",
      "dataset/train/523.PNG\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 13%|██████████▊                                                                        | 6/46 [00:19<02:08,  3.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset/train/073.PNG\n",
      "dataset/train/464.PNG\n",
      "dataset/train/212.PNG\n",
      "dataset/train/203.PNG\n",
      "dataset/train/456.PNG\n",
      "dataset/train/419.PNG\n",
      "dataset/train/095.PNG\n",
      "dataset/train/082.PNG\n",
      "dataset/train/261.PNG\n",
      "dataset/train/358.PNG\n",
      "dataset/train/359.PNG\n",
      "dataset/train/096.PNG\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 15%|████████████▋                                                                      | 7/46 [00:23<02:07,  3.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset/train/451.PNG\n",
      "dataset/train/046.PNG\n",
      "dataset/train/013.PNG\n",
      "dataset/train/038.PNG\n",
      "dataset/train/303.PNG\n",
      "dataset/train/242.PNG\n",
      "dataset/train/466.PNG\n",
      "dataset/train/090.PNG\n",
      "dataset/train/367.PNG\n",
      "dataset/train/403.PNG\n",
      "dataset/train/190.PNG\n",
      "dataset/train/416.PNG\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 17%|██████████████▍                                                                    | 8/46 [00:26<02:01,  3.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset/train/223.PNG\n",
      "dataset/train/050.PNG\n",
      "dataset/train/060.PNG\n",
      "dataset/train/206.PNG\n",
      "dataset/train/048.PNG\n",
      "dataset/train/437.PNG\n",
      "dataset/train/179.PNG\n",
      "dataset/train/173.PNG\n",
      "dataset/train/159.PNG\n",
      "dataset/train/225.PNG\n",
      "dataset/train/539.PNG\n",
      "dataset/train/534.PNG\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 20%|████████████████▏                                                                  | 9/46 [00:29<01:57,  3.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset/train/196.PNG\n",
      "dataset/train/472.PNG\n",
      "dataset/train/351.PNG\n",
      "dataset/train/423.PNG\n",
      "dataset/train/014.PNG\n",
      "dataset/train/287.PNG\n",
      "dataset/train/275.PNG\n",
      "dataset/train/191.PNG\n",
      "dataset/train/244.PNG\n",
      "dataset/train/220.PNG\n",
      "dataset/train/395.PNG\n",
      "dataset/train/033.PNG\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 22%|█████████████████▊                                                                | 10/46 [00:32<01:54,  3.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset/train/295.PNG\n",
      "dataset/train/382.PNG\n",
      "dataset/train/140.PNG\n",
      "dataset/train/228.PNG\n",
      "dataset/train/309.PNG\n",
      "dataset/train/372.PNG\n",
      "dataset/train/168.PNG\n",
      "dataset/train/166.PNG\n",
      "dataset/train/019.PNG\n",
      "dataset/train/185.PNG\n",
      "dataset/train/270.PNG\n",
      "dataset/train/299.PNG\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 24%|███████████████████▌                                                              | 11/46 [00:35<01:50,  3.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset/train/385.PNG\n",
      "dataset/train/350.PNG\n",
      "dataset/train/439.PNG\n",
      "dataset/train/460.PNG\n",
      "dataset/train/415.PNG\n",
      "dataset/train/011.PNG\n",
      "dataset/train/462.PNG\n",
      "dataset/train/102.PNG\n",
      "dataset/train/094.PNG\n",
      "dataset/train/110.PNG\n",
      "dataset/train/006.PNG\n",
      "dataset/train/251.PNG\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 26%|█████████████████████▍                                                            | 12/46 [00:38<01:47,  3.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset/train/128.PNG\n",
      "dataset/train/409.PNG\n",
      "dataset/train/162.PNG\n",
      "dataset/train/235.PNG\n",
      "dataset/train/405.PNG\n",
      "dataset/train/393.PNG\n",
      "dataset/train/352.PNG\n",
      "dataset/train/512.PNG\n",
      "dataset/train/114.PNG\n",
      "dataset/train/227.PNG\n",
      "dataset/train/278.PNG\n",
      "dataset/train/500.PNG\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 28%|███████████████████████▏                                                          | 13/46 [00:42<01:44,  3.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset/train/245.PNG\n",
      "dataset/train/364.PNG\n",
      "dataset/train/047.PNG\n",
      "dataset/train/240.PNG\n",
      "dataset/train/250.PNG\n",
      "dataset/train/376.PNG\n",
      "dataset/train/124.PNG\n",
      "dataset/train/422.PNG\n",
      "dataset/train/012.PNG\n",
      "dataset/train/450.PNG\n",
      "dataset/train/402.PNG\n",
      "dataset/train/496.PNG\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 30%|████████████████████████▉                                                         | 14/46 [00:45<01:39,  3.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset/train/167.PNG\n",
      "dataset/train/027.PNG\n",
      "dataset/train/339.PNG\n",
      "dataset/train/306.PNG\n",
      "dataset/train/045.PNG\n",
      "dataset/train/161.PNG\n",
      "dataset/train/034.PNG\n",
      "dataset/train/207.PNG\n",
      "dataset/train/035.PNG\n",
      "dataset/train/122.PNG\n",
      "dataset/train/044.PNG\n",
      "dataset/train/391.PNG\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 33%|██████████████████████████▋                                                       | 15/46 [00:48<01:35,  3.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset/train/468.PNG\n",
      "dataset/train/218.PNG\n",
      "dataset/train/528.PNG\n",
      "dataset/train/129.PNG\n",
      "dataset/train/408.PNG\n",
      "dataset/train/436.PNG\n",
      "dataset/train/425.PNG\n",
      "dataset/train/316.PNG\n",
      "dataset/train/440.PNG\n",
      "dataset/train/394.PNG\n",
      "dataset/train/193.PNG\n",
      "dataset/train/232.PNG\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 35%|████████████████████████████▌                                                     | 16/46 [00:51<01:31,  3.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset/train/194.PNG\n",
      "dataset/train/105.PNG\n",
      "dataset/train/266.PNG\n",
      "dataset/train/108.PNG\n",
      "dataset/train/355.PNG\n",
      "dataset/train/507.PNG\n",
      "dataset/train/189.PNG\n",
      "dataset/train/106.PNG\n",
      "dataset/train/459.PNG\n",
      "dataset/train/434.PNG\n",
      "dataset/train/356.PNG\n",
      "dataset/train/465.PNG\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 37%|██████████████████████████████▎                                                   | 17/46 [00:54<01:28,  3.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset/train/267.PNG\n",
      "dataset/train/119.PNG\n",
      "dataset/train/086.PNG\n",
      "dataset/train/252.PNG\n",
      "dataset/train/195.PNG\n",
      "dataset/train/224.PNG\n",
      "dataset/train/177.PNG\n",
      "dataset/train/353.PNG\n",
      "dataset/train/363.PNG\n",
      "dataset/train/040.PNG\n",
      "dataset/train/217.PNG\n",
      "dataset/train/504.PNG\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 39%|████████████████████████████████                                                  | 18/46 [00:57<01:24,  3.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset/train/368.PNG\n",
      "dataset/train/484.PNG\n",
      "dataset/train/007.PNG\n",
      "dataset/train/476.PNG\n",
      "dataset/train/186.PNG\n",
      "dataset/train/470.PNG\n",
      "dataset/train/433.PNG\n",
      "dataset/train/180.PNG\n",
      "dataset/train/271.PNG\n",
      "dataset/train/320.PNG\n",
      "dataset/train/479.PNG\n",
      "dataset/train/099.PNG\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 41%|█████████████████████████████████▊                                                | 19/46 [01:00<01:21,  3.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset/train/204.PNG\n",
      "dataset/train/307.PNG\n",
      "dataset/train/282.PNG\n",
      "dataset/train/241.PNG\n",
      "dataset/train/157.PNG\n",
      "dataset/train/155.PNG\n",
      "dataset/train/505.PNG\n",
      "dataset/train/176.PNG\n",
      "dataset/train/302.PNG\n",
      "dataset/train/067.PNG\n",
      "dataset/train/332.PNG\n",
      "dataset/train/308.PNG\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 43%|███████████████████████████████████▋                                              | 20/46 [01:03<01:24,  3.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset/train/065.PNG\n",
      "dataset/train/475.PNG\n",
      "dataset/train/486.PNG\n",
      "dataset/train/325.PNG\n",
      "dataset/train/336.PNG\n",
      "dataset/train/216.PNG\n",
      "dataset/train/183.PNG\n",
      "dataset/train/222.PNG\n",
      "dataset/train/483.PNG\n",
      "dataset/train/215.PNG\n",
      "dataset/train/312.PNG\n",
      "dataset/train/115.PNG\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 46%|█████████████████████████████████████▍                                            | 21/46 [01:06<01:18,  3.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset/train/230.PNG\n",
      "dataset/train/133.PNG\n",
      "dataset/train/330.PNG\n",
      "dataset/train/146.PNG\n",
      "dataset/train/043.PNG\n",
      "dataset/train/369.PNG\n",
      "dataset/train/532.PNG\n",
      "dataset/train/134.PNG\n",
      "dataset/train/139.PNG\n",
      "dataset/train/329.PNG\n",
      "dataset/train/432.PNG\n",
      "dataset/train/042.PNG\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 48%|███████████████████████████████████████▏                                          | 22/46 [01:10<01:16,  3.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset/train/264.PNG\n",
      "dataset/train/092.PNG\n",
      "dataset/train/452.PNG\n",
      "dataset/train/314.PNG\n",
      "dataset/train/390.PNG\n",
      "dataset/train/304.PNG\n",
      "dataset/train/541.PNG\n",
      "dataset/train/070.PNG\n",
      "dataset/train/362.PNG\n",
      "dataset/train/337.PNG\n",
      "dataset/train/493.PNG\n",
      "dataset/train/076.PNG\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 50%|█████████████████████████████████████████                                         | 23/46 [01:13<01:12,  3.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset/train/142.PNG\n",
      "dataset/train/182.PNG\n",
      "dataset/train/342.PNG\n",
      "dataset/train/010.PNG\n",
      "dataset/train/323.PNG\n",
      "dataset/train/163.PNG\n",
      "dataset/train/091.PNG\n",
      "dataset/train/022.PNG\n",
      "dataset/train/262.PNG\n",
      "dataset/train/208.PNG\n",
      "dataset/train/426.PNG\n",
      "dataset/train/083.PNG\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 52%|██████████████████████████████████████████▊                                       | 24/46 [01:16<01:10,  3.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset/train/340.PNG\n",
      "dataset/train/348.PNG\n",
      "dataset/train/529.PNG\n",
      "dataset/train/178.PNG\n",
      "dataset/train/284.PNG\n",
      "dataset/train/268.PNG\n",
      "dataset/train/511.PNG\n",
      "dataset/train/399.PNG\n",
      "dataset/train/061.PNG\n",
      "dataset/train/424.PNG\n",
      "dataset/train/169.PNG\n",
      "dataset/train/198.PNG\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 54%|████████████████████████████████████████████▌                                     | 25/46 [01:19<01:06,  3.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset/train/175.PNG\n",
      "dataset/train/051.PNG\n",
      "dataset/train/238.PNG\n",
      "dataset/train/347.PNG\n",
      "dataset/train/052.PNG\n",
      "dataset/train/136.PNG\n",
      "dataset/train/149.PNG\n",
      "dataset/train/079.PNG\n",
      "dataset/train/097.PNG\n",
      "dataset/train/075.PNG\n",
      "dataset/train/377.PNG\n",
      "dataset/train/089.PNG\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 57%|██████████████████████████████████████████████▎                                   | 26/46 [01:22<01:01,  3.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset/train/380.PNG\n",
      "dataset/train/397.PNG\n",
      "dataset/train/160.PNG\n",
      "dataset/train/187.PNG\n",
      "dataset/train/273.PNG\n",
      "dataset/train/421.PNG\n",
      "dataset/train/401.PNG\n",
      "dataset/train/522.PNG\n",
      "dataset/train/392.PNG\n",
      "dataset/train/509.PNG\n",
      "dataset/train/243.PNG\n",
      "dataset/train/349.PNG\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 59%|████████████████████████████████████████████████▏                                 | 27/46 [01:25<00:57,  3.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset/train/104.PNG\n",
      "dataset/train/326.PNG\n",
      "dataset/train/442.PNG\n",
      "dataset/train/406.PNG\n",
      "dataset/train/429.PNG\n",
      "dataset/train/515.PNG\n",
      "dataset/train/004.PNG\n",
      "dataset/train/236.PNG\n",
      "dataset/train/021.PNG\n",
      "dataset/train/526.PNG\n",
      "dataset/train/374.PNG\n",
      "dataset/train/015.PNG\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 61%|█████████████████████████████████████████████████▉                                | 28/46 [01:28<00:54,  3.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset/train/471.PNG\n",
      "dataset/train/319.PNG\n",
      "dataset/train/305.PNG\n",
      "dataset/train/292.PNG\n",
      "dataset/train/258.PNG\n",
      "dataset/train/109.PNG\n",
      "dataset/train/458.PNG\n",
      "dataset/train/030.PNG\n",
      "dataset/train/265.PNG\n",
      "dataset/train/181.PNG\n",
      "dataset/train/481.PNG\n",
      "dataset/train/431.PNG\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 63%|███████████████████████████████████████████████████▋                              | 29/46 [01:31<00:51,  3.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset/train/492.PNG\n",
      "dataset/train/311.PNG\n",
      "dataset/train/184.PNG\n",
      "dataset/train/322.PNG\n",
      "dataset/train/049.PNG\n",
      "dataset/train/413.PNG\n",
      "dataset/train/026.PNG\n",
      "dataset/train/410.PNG\n",
      "dataset/train/375.PNG\n",
      "dataset/train/087.PNG\n",
      "dataset/train/438.PNG\n",
      "dataset/train/344.PNG\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 65%|█████████████████████████████████████████████████████▍                            | 30/46 [01:34<00:48,  3.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset/train/510.PNG\n",
      "dataset/train/126.PNG\n",
      "dataset/train/260.PNG\n",
      "dataset/train/246.PNG\n",
      "dataset/train/540.PNG\n",
      "dataset/train/418.PNG\n",
      "dataset/train/121.PNG\n",
      "dataset/train/467.PNG\n",
      "dataset/train/188.PNG\n",
      "dataset/train/055.PNG\n",
      "dataset/train/144.PNG\n",
      "dataset/train/018.PNG\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 67%|███████████████████████████████████████████████████████▎                          | 31/46 [01:37<00:46,  3.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset/train/524.PNG\n",
      "dataset/train/116.PNG\n",
      "dataset/train/138.PNG\n",
      "dataset/train/300.PNG\n",
      "dataset/train/032.PNG\n",
      "dataset/train/202.PNG\n",
      "dataset/train/031.PNG\n",
      "dataset/train/384.PNG\n",
      "dataset/train/317.PNG\n",
      "dataset/train/107.PNG\n",
      "dataset/train/463.PNG\n",
      "dataset/train/542.PNG\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 70%|█████████████████████████████████████████████████████████                         | 32/46 [01:40<00:42,  3.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset/train/153.PNG\n",
      "dataset/train/103.PNG\n",
      "dataset/train/283.PNG\n",
      "dataset/train/025.PNG\n",
      "dataset/train/117.PNG\n",
      "dataset/train/135.PNG\n",
      "dataset/train/478.PNG\n",
      "dataset/train/473.PNG\n",
      "dataset/train/020.PNG\n",
      "dataset/train/003.PNG\n",
      "dataset/train/296.PNG\n",
      "dataset/train/396.PNG\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 72%|██████████████████████████████████████████████████████████▊                       | 33/46 [01:43<00:39,  3.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset/train/150.PNG\n",
      "dataset/train/502.PNG\n",
      "dataset/train/229.PNG\n",
      "dataset/train/485.PNG\n",
      "dataset/train/301.PNG\n",
      "dataset/train/255.PNG\n",
      "dataset/train/412.PNG\n",
      "dataset/train/098.PNG\n",
      "dataset/train/477.PNG\n",
      "dataset/train/387.PNG\n",
      "dataset/train/518.PNG\n",
      "dataset/train/370.PNG\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 74%|████████████████████████████████████████████████████████████▌                     | 34/46 [01:46<00:36,  3.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset/train/101.PNG\n",
      "dataset/train/247.PNG\n",
      "dataset/train/009.PNG\n",
      "dataset/train/269.PNG\n",
      "dataset/train/066.PNG\n",
      "dataset/train/276.PNG\n",
      "dataset/train/445.PNG\n",
      "dataset/train/237.PNG\n",
      "dataset/train/286.PNG\n",
      "dataset/train/474.PNG\n",
      "dataset/train/259.PNG\n",
      "dataset/train/536.PNG\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 76%|██████████████████████████████████████████████████████████████▍                   | 35/46 [01:49<00:32,  2.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset/train/062.PNG\n",
      "dataset/train/277.PNG\n",
      "dataset/train/514.PNG\n",
      "dataset/train/233.PNG\n",
      "dataset/train/171.PNG\n",
      "dataset/train/469.PNG\n",
      "dataset/train/205.PNG\n",
      "dataset/train/111.PNG\n",
      "dataset/train/499.PNG\n",
      "dataset/train/290.PNG\n",
      "dataset/train/281.PNG\n",
      "dataset/train/201.PNG\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 78%|████████████████████████████████████████████████████████████████▏                 | 36/46 [01:52<00:30,  3.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset/train/346.PNG\n",
      "dataset/train/520.PNG\n",
      "dataset/train/449.PNG\n",
      "dataset/train/531.PNG\n",
      "dataset/train/537.PNG\n",
      "dataset/train/219.PNG\n",
      "dataset/train/457.PNG\n",
      "dataset/train/213.PNG\n",
      "dataset/train/448.PNG\n",
      "dataset/train/125.PNG\n",
      "dataset/train/132.PNG\n",
      "dataset/train/331.PNG\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 80%|█████████████████████████████████████████████████████████████████▉                | 37/46 [01:55<00:27,  3.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset/train/420.PNG\n",
      "dataset/train/508.PNG\n",
      "dataset/train/313.PNG\n",
      "dataset/train/174.PNG\n",
      "dataset/train/005.PNG\n",
      "dataset/train/145.PNG\n",
      "dataset/train/272.PNG\n",
      "dataset/train/354.PNG\n",
      "dataset/train/366.PNG\n",
      "dataset/train/297.PNG\n",
      "dataset/train/383.PNG\n",
      "dataset/train/063.PNG\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 83%|███████████████████████████████████████████████████████████████████▋              | 38/46 [01:58<00:24,  3.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset/train/141.PNG\n",
      "dataset/train/453.PNG\n",
      "dataset/train/081.PNG\n",
      "dataset/train/257.PNG\n",
      "dataset/train/446.PNG\n",
      "dataset/train/379.PNG\n",
      "dataset/train/480.PNG\n",
      "dataset/train/064.PNG\n",
      "dataset/train/400.PNG\n",
      "dataset/train/288.PNG\n",
      "dataset/train/321.PNG\n",
      "dataset/train/289.PNG\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 85%|█████████████████████████████████████████████████████████████████████▌            | 39/46 [02:01<00:21,  3.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset/train/398.PNG\n",
      "dataset/train/506.PNG\n",
      "dataset/train/147.PNG\n",
      "dataset/train/310.PNG\n",
      "dataset/train/080.PNG\n",
      "dataset/train/209.PNG\n",
      "dataset/train/151.PNG\n",
      "dataset/train/256.PNG\n",
      "dataset/train/335.PNG\n",
      "dataset/train/248.PNG\n",
      "dataset/train/164.PNG\n",
      "dataset/train/039.PNG\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 87%|███████████████████████████████████████████████████████████████████████▎          | 40/46 [02:05<00:19,  3.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset/train/389.PNG\n",
      "dataset/train/056.PNG\n",
      "dataset/train/059.PNG\n",
      "dataset/train/172.PNG\n",
      "dataset/train/318.PNG\n",
      "dataset/train/341.PNG\n",
      "dataset/train/488.PNG\n",
      "dataset/train/069.PNG\n",
      "dataset/train/293.PNG\n",
      "dataset/train/084.PNG\n",
      "dataset/train/071.PNG\n",
      "dataset/train/214.PNG\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 89%|█████████████████████████████████████████████████████████████████████████         | 41/46 [02:08<00:15,  3.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset/train/152.PNG\n",
      "dataset/train/519.PNG\n",
      "dataset/train/378.PNG\n",
      "dataset/train/058.PNG\n",
      "dataset/train/029.PNG\n",
      "dataset/train/501.PNG\n",
      "dataset/train/156.PNG\n",
      "dataset/train/037.PNG\n",
      "dataset/train/210.PNG\n",
      "dataset/train/482.PNG\n",
      "dataset/train/239.PNG\n",
      "dataset/train/017.PNG\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 91%|██████████████████████████████████████████████████████████████████████████▊       | 42/46 [02:11<00:12,  3.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset/train/078.PNG\n",
      "dataset/train/263.PNG\n",
      "dataset/train/324.PNG\n",
      "dataset/train/447.PNG\n",
      "dataset/train/234.PNG\n",
      "dataset/train/361.PNG\n",
      "dataset/train/441.PNG\n",
      "dataset/train/497.PNG\n",
      "dataset/train/068.PNG\n",
      "dataset/train/343.PNG\n",
      "dataset/train/148.PNG\n",
      "dataset/train/036.PNG\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 93%|████████████████████████████████████████████████████████████████████████████▋     | 43/46 [02:14<00:09,  3.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset/train/489.PNG\n",
      "dataset/train/417.PNG\n",
      "dataset/train/345.PNG\n",
      "dataset/train/454.PNG\n",
      "dataset/train/254.PNG\n",
      "dataset/train/016.PNG\n",
      "dataset/train/327.PNG\n",
      "dataset/train/131.PNG\n",
      "dataset/train/490.PNG\n",
      "dataset/train/334.PNG\n",
      "dataset/train/041.PNG\n",
      "dataset/train/443.PNG\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 96%|██████████████████████████████████████████████████████████████████████████████▍   | 44/46 [02:17<00:06,  3.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset/train/435.PNG\n",
      "dataset/train/008.PNG\n",
      "dataset/train/428.PNG\n",
      "dataset/train/407.PNG\n",
      "dataset/train/057.PNG\n",
      "dataset/train/279.PNG\n",
      "dataset/train/130.PNG\n",
      "dataset/train/112.PNG\n",
      "dataset/train/274.PNG\n",
      "dataset/train/200.PNG\n",
      "dataset/train/093.PNG\n",
      "dataset/train/373.PNG\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 98%|████████████████████████████████████████████████████████████████████████████████▏ | 45/46 [02:20<00:03,  3.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset/train/280.PNG\n",
      "dataset/train/199.PNG\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 46/46 [02:21<00:00,  3.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] Train loss: 5.0156815259\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                           | 0/16 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset/train/614.PNG\n",
      "dataset/train/625.PNG\n",
      "dataset/train/545.PNG\n",
      "dataset/train/657.PNG\n",
      "dataset/train/639.PNG\n",
      "dataset/train/647.PNG\n",
      "dataset/train/709.PNG\n",
      "dataset/train/634.PNG\n",
      "dataset/train/568.PNG\n",
      "dataset/train/640.PNG\n",
      "dataset/train/553.PNG\n",
      "dataset/train/589.PNG\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  6%|█████▏                                                                             | 1/16 [00:01<00:15,  1.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset/train/620.PNG\n",
      "dataset/train/616.PNG\n",
      "dataset/train/695.PNG\n",
      "dataset/train/577.PNG\n",
      "dataset/train/610.PNG\n",
      "dataset/train/668.PNG\n",
      "dataset/train/559.PNG\n",
      "dataset/train/628.PNG\n",
      "dataset/train/619.PNG\n",
      "dataset/train/569.PNG\n",
      "dataset/train/666.PNG\n",
      "dataset/train/691.PNG\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 12%|██████████▍                                                                        | 2/16 [00:02<00:15,  1.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset/train/712.PNG\n",
      "dataset/train/633.PNG\n",
      "dataset/train/571.PNG\n",
      "dataset/train/546.PNG\n",
      "dataset/train/599.PNG\n",
      "dataset/train/682.PNG\n",
      "dataset/train/648.PNG\n",
      "dataset/train/636.PNG\n",
      "dataset/train/653.PNG\n",
      "dataset/train/597.PNG\n",
      "dataset/train/632.PNG\n",
      "dataset/train/720.PNG\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 19%|███████████████▌                                                                   | 3/16 [00:03<00:15,  1.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset/train/664.PNG\n",
      "dataset/train/622.PNG\n",
      "dataset/train/578.PNG\n",
      "dataset/train/572.PNG\n",
      "dataset/train/574.PNG\n",
      "dataset/train/719.PNG\n",
      "dataset/train/663.PNG\n",
      "dataset/train/690.PNG\n",
      "dataset/train/649.PNG\n",
      "dataset/train/600.PNG\n",
      "dataset/train/698.PNG\n",
      "dataset/train/708.PNG\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 25%|████████████████████▊                                                              | 4/16 [00:04<00:14,  1.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset/train/566.PNG\n",
      "dataset/train/609.PNG\n",
      "dataset/train/630.PNG\n",
      "dataset/train/641.PNG\n",
      "dataset/train/714.PNG\n",
      "dataset/train/723.PNG\n",
      "dataset/train/679.PNG\n",
      "dataset/train/651.PNG\n",
      "dataset/train/621.PNG\n",
      "dataset/train/717.PNG\n",
      "dataset/train/638.PNG\n",
      "dataset/train/715.PNG\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 31%|█████████████████████████▉                                                         | 5/16 [00:05<00:12,  1.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset/train/631.PNG\n",
      "dataset/train/677.PNG\n",
      "dataset/train/615.PNG\n",
      "dataset/train/684.PNG\n",
      "dataset/train/580.PNG\n",
      "dataset/train/694.PNG\n",
      "dataset/train/722.PNG\n",
      "dataset/train/548.PNG\n",
      "dataset/train/601.PNG\n",
      "dataset/train/596.PNG\n",
      "dataset/train/646.PNG\n",
      "dataset/train/587.PNG\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 38%|███████████████████████████████▏                                                   | 6/16 [00:06<00:11,  1.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset/train/642.PNG\n",
      "dataset/train/588.PNG\n",
      "dataset/train/716.PNG\n",
      "dataset/train/660.PNG\n",
      "dataset/train/570.PNG\n",
      "dataset/train/593.PNG\n",
      "dataset/train/558.PNG\n",
      "dataset/train/655.PNG\n",
      "dataset/train/585.PNG\n",
      "dataset/train/544.PNG\n",
      "dataset/train/555.PNG\n",
      "dataset/train/594.PNG\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 44%|████████████████████████████████████▎                                              | 7/16 [00:08<00:10,  1.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset/train/608.PNG\n",
      "dataset/train/652.PNG\n",
      "dataset/train/592.PNG\n",
      "dataset/train/635.PNG\n",
      "dataset/train/595.PNG\n",
      "dataset/train/658.PNG\n",
      "dataset/train/643.PNG\n",
      "dataset/train/718.PNG\n",
      "dataset/train/702.PNG\n",
      "dataset/train/644.PNG\n",
      "dataset/train/645.PNG\n",
      "dataset/train/669.PNG\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 50%|█████████████████████████████████████████▌                                         | 8/16 [00:09<00:09,  1.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset/train/581.PNG\n",
      "dataset/train/696.PNG\n",
      "dataset/train/584.PNG\n",
      "dataset/train/711.PNG\n",
      "dataset/train/713.PNG\n",
      "dataset/train/659.PNG\n",
      "dataset/train/554.PNG\n",
      "dataset/train/656.PNG\n",
      "dataset/train/549.PNG\n",
      "dataset/train/563.PNG\n",
      "dataset/train/575.PNG\n",
      "dataset/train/607.PNG\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 56%|██████████████████████████████████████████████▋                                    | 9/16 [00:10<00:08,  1.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset/train/623.PNG\n",
      "dataset/train/603.PNG\n",
      "dataset/train/547.PNG\n",
      "dataset/train/618.PNG\n",
      "dataset/train/579.PNG\n",
      "dataset/train/699.PNG\n",
      "dataset/train/617.PNG\n",
      "dataset/train/665.PNG\n",
      "dataset/train/650.PNG\n",
      "dataset/train/675.PNG\n",
      "dataset/train/591.PNG\n",
      "dataset/train/604.PNG\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 62%|███████████████████████████████████████████████████▎                              | 10/16 [00:11<00:07,  1.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset/train/693.PNG\n",
      "dataset/train/586.PNG\n",
      "dataset/train/565.PNG\n",
      "dataset/train/573.PNG\n",
      "dataset/train/680.PNG\n",
      "dataset/train/662.PNG\n",
      "dataset/train/612.PNG\n",
      "dataset/train/683.PNG\n",
      "dataset/train/576.PNG\n",
      "dataset/train/674.PNG\n",
      "dataset/train/557.PNG\n",
      "dataset/train/567.PNG\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 69%|████████████████████████████████████████████████████████▍                         | 11/16 [00:12<00:05,  1.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset/train/721.PNG\n",
      "dataset/train/706.PNG\n",
      "dataset/train/705.PNG\n",
      "dataset/train/692.PNG\n",
      "dataset/train/611.PNG\n",
      "dataset/train/543.PNG\n",
      "dataset/train/654.PNG\n",
      "dataset/train/626.PNG\n",
      "dataset/train/671.PNG\n",
      "dataset/train/627.PNG\n",
      "dataset/train/681.PNG\n",
      "dataset/train/670.PNG\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 75%|█████████████████████████████████████████████████████████████▌                    | 12/16 [00:14<00:04,  1.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset/train/672.PNG\n",
      "dataset/train/703.PNG\n",
      "dataset/train/606.PNG\n",
      "dataset/train/667.PNG\n",
      "dataset/train/552.PNG\n",
      "dataset/train/685.PNG\n",
      "dataset/train/704.PNG\n",
      "dataset/train/688.PNG\n",
      "dataset/train/710.PNG\n",
      "dataset/train/583.PNG\n",
      "dataset/train/678.PNG\n",
      "dataset/train/689.PNG\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 81%|██████████████████████████████████████████████████████████████████▋               | 13/16 [00:15<00:03,  1.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset/train/582.PNG\n",
      "dataset/train/687.PNG\n",
      "dataset/train/701.PNG\n",
      "dataset/train/605.PNG\n",
      "dataset/train/697.PNG\n",
      "dataset/train/598.PNG\n",
      "dataset/train/707.PNG\n",
      "dataset/train/590.PNG\n",
      "dataset/train/676.PNG\n",
      "dataset/train/629.PNG\n",
      "dataset/train/551.PNG\n",
      "dataset/train/613.PNG\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 88%|███████████████████████████████████████████████████████████████████████▊          | 14/16 [00:16<00:02,  1.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset/train/624.PNG\n",
      "dataset/train/550.PNG\n",
      "dataset/train/700.PNG\n",
      "dataset/train/661.PNG\n",
      "dataset/train/564.PNG\n",
      "dataset/train/673.PNG\n",
      "dataset/train/562.PNG\n",
      "dataset/train/602.PNG\n",
      "dataset/train/686.PNG\n",
      "dataset/train/556.PNG\n",
      "dataset/train/561.PNG\n",
      "dataset/train/560.PNG\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 16/16 [00:17<00:00,  1.09s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset/train/637.PNG\n",
      "Vail set: Loss: 5.8746, Accuracy: 17/181 ( 9%)\n",
      "\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/save_data/saved/best_model.pth'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [103]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [101]\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, optimizer, train_loader, scheduler, device)\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m best_acc \u001b[38;5;241m<\u001b[39m vali_acc:\n\u001b[0;32m     47\u001b[0m     best_acc \u001b[38;5;241m=\u001b[39m vali_acc\n\u001b[1;32m---> 48\u001b[0m     \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstate_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdata/save_data/saved/best_model.pth\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m#이 디렉토리에 best_model.pth을 저장\u001b[39;00m\n\u001b[0;32m     49\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mModel Saved.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\serialization.py:376\u001b[0m, in \u001b[0;36msave\u001b[1;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization)\u001b[0m\n\u001b[0;32m    340\u001b[0m \u001b[38;5;124;03m\"\"\"save(obj, f, pickle_module=pickle, pickle_protocol=DEFAULT_PROTOCOL, _use_new_zipfile_serialization=True)\u001b[39;00m\n\u001b[0;32m    341\u001b[0m \n\u001b[0;32m    342\u001b[0m \u001b[38;5;124;03mSaves an object to a disk file.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    372\u001b[0m \u001b[38;5;124;03m    >>> torch.save(x, buffer)\u001b[39;00m\n\u001b[0;32m    373\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    374\u001b[0m _check_dill_version(pickle_module)\n\u001b[1;32m--> 376\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_file_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mwb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[0;32m    377\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _use_new_zipfile_serialization:\n\u001b[0;32m    378\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m _open_zipfile_writer(opened_file) \u001b[38;5;28;01mas\u001b[39;00m opened_zipfile:\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\serialization.py:230\u001b[0m, in \u001b[0;36m_open_file_like\u001b[1;34m(name_or_buffer, mode)\u001b[0m\n\u001b[0;32m    228\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[0;32m    229\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[1;32m--> 230\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_open_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    231\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    232\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\serialization.py:211\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[1;34m(self, name, mode)\u001b[0m\n\u001b[0;32m    210\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, mode):\n\u001b[1;32m--> 211\u001b[0m     \u001b[38;5;28msuper\u001b[39m(_open_file, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/save_data/saved/best_model.pth'"
     ]
    }
   ],
   "source": [
    "train(model, optimizer, train_loader, scheduler, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05c3f92d",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a22ad6e",
   "metadata": {},
   "source": [
    "[Vali set] Avg Loss: 0.9572, Accuracy: 140/181 ( 77%)\n",
    "\n",
    "Model Saved."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaee8f33",
   "metadata": {},
   "source": [
    "**Epoch가 [   ]일 때 Validation Accuracy가 [  ]%로 best_model에 선정되어 저장되었다.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34216eb9",
   "metadata": {},
   "source": [
    "## 3. 추론하기 (Inference) - Test 데이터에 적용하기\n",
    "\n",
    "이제 학습된 best_model을 가지고 test 셋의 라벨을 추론해보도록 하자.\n",
    "\n",
    "`with torch.no_grad():`\n",
    "\n",
    ": Autograd 엔진을 꺼버려 더이상 자동으로 gradient 트래킹을 하지 않는다. 이렇게 함으로써 메모리 사용량을 줄이고 연산 속도를 높히기 위함이다. \n",
    " \n",
    " <br/>\n",
    "\n",
    "`model.eval()`\n",
    "\n",
    ": train 데이터를 활용하는 모델링 단계와 test 데이터로 추론하는 단계에서 다르게 동작하는 layer들이 존재한다. 예를 들면, Dropout layer나 BatchNorm 같은 레이어는 학습 시에만 동작한다. \n",
    "\n",
    "이런 layer들의 동작을 inference(evaluation) mode로 바꿔주는 목적으로 사용된다. \n",
    "\n",
    "**따라서, inference를 진행할 때에는 두가지 메소드를 다 호출해야한다!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "99b5aef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model,test_loader,device):\n",
    "    model.eval()  # evaluation 과정에서 사용하지 않아야 하는 layer들을 알아서 off 시킴\n",
    "    model_pred = []\n",
    "    with torch.no_grad():\n",
    "        for img in tqdm(iter(test_loader)):\n",
    "            img = img.to(device)\n",
    "            \n",
    "            pred_logit = model(img)\n",
    "            pred_logit = pred_logit.argmax(dim =1, keepdim = True).squeeze(1)\n",
    "            \n",
    "            model_pred.extend(pred_logit.tolist())\n",
    "    return model_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "16506950",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                           | 0/17 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset/test/001.PNG\n",
      "dataset/test/002.PNG\n",
      "dataset/test/003.PNG\n",
      "dataset/test/004.PNG\n",
      "dataset/test/005.PNG\n",
      "dataset/test/006.PNG\n",
      "dataset/test/007.PNG\n",
      "dataset/test/008.PNG\n",
      "dataset/test/009.PNG\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  6%|████▉                                                                              | 1/17 [00:00<00:05,  3.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset/test/010.PNG\n",
      "dataset/test/011.PNG\n",
      "dataset/test/012.PNG\n",
      "dataset/test/013.PNG\n",
      "dataset/test/014.PNG\n",
      "dataset/test/015.PNG\n",
      "dataset/test/016.PNG\n",
      "dataset/test/017.PNG\n",
      "dataset/test/018.PNG\n",
      "dataset/test/019.PNG\n",
      "dataset/test/020.PNG\n",
      "dataset/test/021.PNG\n",
      "dataset/test/022.PNG\n",
      "dataset/test/023.PNG\n",
      "dataset/test/024.PNG\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 12%|█████████▊                                                                         | 2/17 [00:00<00:05,  2.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset/test/025.PNG\n",
      "dataset/test/026.PNG\n",
      "dataset/test/027.PNG\n",
      "dataset/test/028.PNG\n",
      "dataset/test/029.PNG\n",
      "dataset/test/030.PNG\n",
      "dataset/test/031.PNG\n",
      "dataset/test/032.PNG\n",
      "dataset/test/033.PNG\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 18%|██████████████▋                                                                    | 3/17 [00:01<00:04,  2.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset/test/034.PNG\n",
      "dataset/test/035.PNG\n",
      "dataset/test/036.PNG\n",
      "dataset/test/037.PNG\n",
      "dataset/test/038.PNG\n",
      "dataset/test/039.PNG\n",
      "dataset/test/040.PNG\n",
      "dataset/test/041.PNG\n",
      "dataset/test/042.PNG\n",
      "dataset/test/043.PNG\n",
      "dataset/test/044.PNG\n",
      "dataset/test/045.PNG\n",
      "dataset/test/046.PNG\n",
      "dataset/test/047.PNG\n",
      "dataset/test/048.PNG\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 24%|███████████████████▌                                                               | 4/17 [00:01<00:04,  2.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset/test/049.PNG\n",
      "dataset/test/050.PNG\n",
      "dataset/test/051.PNG\n",
      "dataset/test/052.PNG\n",
      "dataset/test/053.PNG\n",
      "dataset/test/054.PNG\n",
      "dataset/test/055.PNG\n",
      "dataset/test/056.PNG\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 29%|████████████████████████▍                                                          | 5/17 [00:01<00:04,  2.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset/test/057.PNG\n",
      "dataset/test/058.PNG\n",
      "dataset/test/059.PNG\n",
      "dataset/test/060.PNG\n",
      "dataset/test/061.PNG\n",
      "dataset/test/062.PNG\n",
      "dataset/test/063.PNG\n",
      "dataset/test/064.PNG\n",
      "dataset/test/065.PNG\n",
      "dataset/test/066.PNG\n",
      "dataset/test/067.PNG\n",
      "dataset/test/068.PNG\n",
      "dataset/test/069.PNG\n",
      "dataset/test/070.PNG\n",
      "dataset/test/071.PNG\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 35%|█████████████████████████████▎                                                     | 6/17 [00:02<00:03,  2.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset/test/072.PNG\n",
      "dataset/test/073.PNG\n",
      "dataset/test/074.PNG\n",
      "dataset/test/075.PNG\n",
      "dataset/test/076.PNG\n",
      "dataset/test/077.PNG\n",
      "dataset/test/078.PNG\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 41%|██████████████████████████████████▏                                                | 7/17 [00:02<00:03,  2.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset/test/079.PNG\n",
      "dataset/test/080.PNG\n",
      "dataset/test/081.PNG\n",
      "dataset/test/082.PNG\n",
      "dataset/test/083.PNG\n",
      "dataset/test/084.PNG\n",
      "dataset/test/085.PNG\n",
      "dataset/test/086.PNG\n",
      "dataset/test/087.PNG\n",
      "dataset/test/088.PNG\n",
      "dataset/test/089.PNG\n",
      "dataset/test/090.PNG\n",
      "dataset/test/091.PNG\n",
      "dataset/test/092.PNG\n",
      "dataset/test/093.PNG\n",
      "dataset/test/094.PNG\n",
      "dataset/test/095.PNG\n",
      "dataset/test/096.PNG\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 47%|███████████████████████████████████████                                            | 8/17 [00:02<00:03,  2.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset/test/097.PNG\n",
      "dataset/test/098.PNG\n",
      "dataset/test/099.PNG\n",
      "dataset/test/100.PNG\n",
      "dataset/test/101.PNG\n",
      "dataset/test/102.PNG\n",
      "dataset/test/103.PNG\n",
      "dataset/test/104.PNG\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 53%|███████████████████████████████████████████▉                                       | 9/17 [00:03<00:03,  2.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset/test/105.PNG\n",
      "dataset/test/106.PNG\n",
      "dataset/test/107.PNG\n",
      "dataset/test/108.PNG\n",
      "dataset/test/109.PNG\n",
      "dataset/test/110.PNG\n",
      "dataset/test/111.PNG\n",
      "dataset/test/112.PNG\n",
      "dataset/test/113.PNG\n",
      "dataset/test/114.PNG\n",
      "dataset/test/115.PNG\n",
      "dataset/test/116.PNG\n",
      "dataset/test/117.PNG\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 59%|████████████████████████████████████████████████▏                                 | 10/17 [00:03<00:02,  2.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset/test/118.PNG\n",
      "dataset/test/119.PNG\n",
      "dataset/test/120.PNG\n",
      "dataset/test/121.PNG\n",
      "dataset/test/122.PNG\n",
      "dataset/test/123.PNG\n",
      "dataset/test/124.PNG\n",
      "dataset/test/125.PNG\n",
      "dataset/test/126.PNG\n",
      "dataset/test/127.PNG\n",
      "dataset/test/128.PNG\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 65%|█████████████████████████████████████████████████████                             | 11/17 [00:04<00:02,  2.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset/test/129.PNG\n",
      "dataset/test/130.PNG\n",
      "dataset/test/131.PNG\n",
      "dataset/test/132.PNG\n",
      "dataset/test/133.PNG\n",
      "dataset/test/134.PNG\n",
      "dataset/test/135.PNG\n",
      "dataset/test/136.PNG\n",
      "dataset/test/137.PNG\n",
      "dataset/test/138.PNG\n",
      "dataset/test/139.PNG\n",
      "dataset/test/140.PNG\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 71%|█████████████████████████████████████████████████████████▉                        | 12/17 [00:04<00:01,  2.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset/test/141.PNG\n",
      "dataset/test/142.PNG\n",
      "dataset/test/143.PNG\n",
      "dataset/test/144.PNG\n",
      "dataset/test/145.PNG\n",
      "dataset/test/146.PNG\n",
      "dataset/test/147.PNG\n",
      "dataset/test/148.PNG\n",
      "dataset/test/149.PNG\n",
      "dataset/test/150.PNG\n",
      "dataset/test/151.PNG\n",
      "dataset/test/152.PNG\n",
      "dataset/test/153.PNG\n",
      "dataset/test/154.PNG\n",
      "dataset/test/155.PNG\n",
      "dataset/test/156.PNG\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|█████████████████████████████████████████████████████████▉                        | 12/17 [00:04<00:02,  2.49it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [31]\u001b[0m, in \u001b[0;36m<cell line: 19>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     16\u001b[0m model\u001b[38;5;241m.\u001b[39mload_state_dict(checkpoint)\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# Inference\u001b[39;00m\n\u001b[1;32m---> 19\u001b[0m preds \u001b[38;5;241m=\u001b[39m \u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     20\u001b[0m preds[\u001b[38;5;241m0\u001b[39m:\u001b[38;5;241m5\u001b[39m]\n",
      "Input \u001b[1;32mIn [30]\u001b[0m, in \u001b[0;36mpredict\u001b[1;34m(model, test_loader, device)\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m img \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28miter\u001b[39m(test_loader)):\n\u001b[0;32m      6\u001b[0m     img \u001b[38;5;241m=\u001b[39m img\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m----> 8\u001b[0m     pred_logit \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m     pred_logit \u001b[38;5;241m=\u001b[39m pred_logit\u001b[38;5;241m.\u001b[39margmax(dim \u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, keepdim \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m)\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     11\u001b[0m     model_pred\u001b[38;5;241m.\u001b[39mextend(pred_logit\u001b[38;5;241m.\u001b[39mtolist())\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Input \u001b[1;32mIn [25]\u001b[0m, in \u001b[0;36mCNNclassification.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m     34\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer1(x) \u001b[38;5;66;03m#1층\u001b[39;00m\n\u001b[1;32m---> 36\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m#2층\u001b[39;00m\n\u001b[0;32m     38\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer3(x) \u001b[38;5;66;03m#3층\u001b[39;00m\n\u001b[0;32m     40\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer4(x) \u001b[38;5;66;03m#4층\u001b[39;00m\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\container.py:139\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    137\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    138\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 139\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    140\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\activation.py:98\u001b[0m, in \u001b[0;36mReLU.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     97\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m---> 98\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrelu\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1457\u001b[0m, in \u001b[0;36mrelu\u001b[1;34m(input, inplace)\u001b[0m\n\u001b[0;32m   1455\u001b[0m     result \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrelu_(\u001b[38;5;28minput\u001b[39m)\n\u001b[0;32m   1456\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1457\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrelu\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1458\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "test_dataset = CustomDataset(test_img_path,\n",
    "                             None,\n",
    "                             train_mode = False,\n",
    "                             transforms= test_transform\n",
    "                            )\n",
    "test_loader = DataLoader(test_dataset,\n",
    "                        batch_size = CFG['BATCH_SIZE'],\n",
    "                         shuffle = False,\n",
    "                         num_workers = 0\n",
    "                        )\n",
    "\n",
    "\n",
    "# Validation Accuracy가 가장 뛰|어난 모델을 불러온다\n",
    "checkpoint = torch.load('best_model.pth')\n",
    "model = CNNclassification().to(device)\n",
    "model.load_state_dict(checkpoint)\n",
    "\n",
    "# Inference\n",
    "preds = predict(model, test_loader, device)\n",
    "preds[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "325ef014",
   "metadata": {},
   "source": [
    "# IV. 데이콘 제출하기\n",
    "\n",
    "## 1. Submission 파일 생성\n",
    "\n",
    "이제 예측한 값을 sample_submission.csv에 넣어 제출용 파일을 생성하자.\n",
    "제출한 뒤 리더보드를 통해 결과 확인."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9a2c145e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'preds' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [32]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m submission  \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdataset/sample_submission.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m submission[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mpreds\u001b[49m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'preds' is not defined"
     ]
    }
   ],
   "source": [
    "submission  = pd.read_csv('dataset/sample_submission.csv')\n",
    "submission['label'] = preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "d888c6bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_name</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>001.PNG</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>002.PNG</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>003.PNG</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>004.PNG</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>005.PNG</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  file_name  label\n",
       "0   001.PNG      7\n",
       "1   002.PNG      1\n",
       "2   003.PNG      9\n",
       "3   004.PNG      7\n",
       "4   005.PNG      6"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "8df1291e",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv('submission/submission1.csv',index=False)  \n",
    "#index False 꼭 넣기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7c2602a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv('submission/submission1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "cca831c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 199 entries, 0 to 198\n",
      "Data columns (total 2 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   file_name  199 non-null    object\n",
      " 1   label      199 non-null    int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 3.2+ KB\n"
     ]
    }
   ],
   "source": [
    "submission.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d6474922",
   "metadata": {},
   "outputs": [],
   "source": [
    "samplesubmission = pd.read_csv('dataset/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "bd656707",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 199 entries, 0 to 198\n",
      "Data columns (total 2 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   file_name  199 non-null    object\n",
      " 1   label      199 non-null    int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 3.2+ KB\n"
     ]
    }
   ],
   "source": [
    "samplesubmission.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b70817d8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
