{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e8d2bc5",
   "metadata": {},
   "source": [
    "# 데이콘 Basic Summer\n",
    "## 서울 랜드마크 이미지 분류 경진대회"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d85fe1ad",
   "metadata": {},
   "source": [
    "\n",
    "# I. 데이터 살펴보기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d240209b",
   "metadata": {},
   "source": [
    "## 1. 데이터 준비\n",
    "### 1.1 csv 데이터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "656b1b27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_name</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>001.PNG</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>002.PNG</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>003.PNG</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>004.PNG</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>005.PNG</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  file_name  label\n",
       "0   001.PNG      9\n",
       "1   002.PNG      4\n",
       "2   003.PNG      1\n",
       "3   004.PNG      1\n",
       "4   005.PNG      6"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pandas 사용하여 데이터 불러오기\n",
    "\n",
    "import pandas as pd\n",
    "label_df = pd.read_csv('dataset/train.csv')\n",
    "label_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4463066",
   "metadata": {},
   "source": [
    "### 1.2 이미지 데이터\n",
    "\n",
    "데이터 이미지의 local address와 label 값을 list에 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "18417f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "# CNN 모델에서 꼭 필요한 두가지 모듈\n",
    "# 'glob' : 파일의 경로명을 리스트로 뽑을 때 사용\n",
    "\n",
    "def get_train_data(data_dir):\n",
    "    img_path_list = []\n",
    "    label_list = []\n",
    "    \n",
    "    # get image path\n",
    "    img_path_list.extend(glob(os.path.join(data_dir,'*.PNG')))\n",
    "    img_path_list = list(map(lambda x: x.replace('\\\\','/', 10), img_path_list))\n",
    "    img_path_list.sort(key=lambda x:int(x.split('/')[-1].split('.')[0]))\n",
    "\n",
    "    # get label\n",
    "    label_list.extend(label_df['label'])\n",
    "    \n",
    "    #print(img_path_list)\n",
    "    #print(label_list)\n",
    "    \n",
    "    return img_path_list,label_list\n",
    "\n",
    "def get_test_data(data_dir):\n",
    "    img_path_list = []\n",
    "    \n",
    "    # get image path\n",
    "    img_path_list.extend(glob(os.path.join(data_dir,'*.PNG')))\n",
    "    img_path_list = list(map(lambda x: x.replace('\\\\','/', 10), img_path_list))\n",
    "    img_path_list.sort(key=lambda x: int(x.split('/')[-1].split('.')[0]))\n",
    "    #print(img_path_list)\n",
    "    \n",
    "    return img_path_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c052bc5",
   "metadata": {},
   "source": [
    "## 2. 데이터 확인\n",
    "### 2.1. csv 데이터\n",
    "\n",
    "pandas의 `info()` 메소드 활용하여 데이터의 특성 살펴보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "339c9099",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 723 entries, 0 to 722\n",
      "Data columns (total 2 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   file_name  723 non-null    object\n",
      " 1   label      723 non-null    int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 11.4+ KB\n"
     ]
    }
   ],
   "source": [
    "label_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "280e7c62",
   "metadata": {},
   "source": [
    "### 2.2. 이미지 데이터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f6f29d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_img_path, all_label = get_train_data('dataset/train')\n",
    "test_img_path = get_test_data('dataset/test')\n",
    "\n",
    "#['dataset/train/001.PNG',\n",
    "#['dataset/train\\\\001.PNG',"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2f107609",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[9, 4, 1, 1, 6]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_label[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "72c72c6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dataset/train/001.PNG',\n",
       " 'dataset/train/002.PNG',\n",
       " 'dataset/train/003.PNG',\n",
       " 'dataset/train/004.PNG',\n",
       " 'dataset/train/005.PNG']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_img_path[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fb07a324",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dataset/test/001.PNG',\n",
       " 'dataset/test/002.PNG',\n",
       " 'dataset/test/003.PNG',\n",
       " 'dataset/test/004.PNG',\n",
       " 'dataset/test/005.PNG']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_img_path[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b2aa759",
   "metadata": {},
   "source": [
    "## 3. 환경설정\n",
    "데이터를 전처리 하기위함 GPU 딥러닝 환경설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "049d8513",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "os.environ[\"CUDA_DEVISE_ORDER\"] = \"PCI_BUS_ID\"  # rrange GPU devices starting from 0\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\"  # Set the GPU 2 to use, 멀티 GPU\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "02c2c738",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6a091865",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU availavle, using the CPU instead.\n"
     ]
    }
   ],
   "source": [
    "# GPU 체크 및 할당\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    #device = torch.device(\"cuda:0\")\n",
    "    print(\"Device:\",device)\n",
    "    print(\"There are %d GPU(s) available\"%torch.cuda.device_count())\n",
    "    print(\"We will use the GPU:\",torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"No GPU availavle, using the CPU instead.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d72885be",
   "metadata": {},
   "source": [
    "# 똥컴이라 안됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0be6278b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.12.1\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(torch.__version__)\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "feac0c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 하이퍼 파라미터 튜닝\n",
    "CFG = {\n",
    "    'IMG_SIZE':128, # 이미지 사이즈\n",
    "    'EPOCHS':50, # 에포크 - 전체 데이터를 사용하여 학습하는 횟수\n",
    "    'LEARNING_RATE':2e-2, # 학습률\n",
    "    'BATCH_SIZE':12, #배치사이즈\n",
    "    'SEED':41, #시드\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3265226a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델의 재현성을 위해 random seed 고정\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    #torch.bakcends.cudnn.deterministic = True\n",
    "    #torch.backends cudnn benchmark = True\n",
    "    \n",
    "seed_everything(CFG['SEED'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67a42912",
   "metadata": {},
   "source": [
    "# II. 데이터 전처리\n",
    "## 2. CustomDataset\n",
    "CustomDataset을 만들어 전체 dataset을 구성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8e5be548",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.datasets as datasets  # 이미지 데이터셋 집합체\n",
    "import torchvision.transforms as transforms  # 이미지 변환 툴 - 텐서 변환, 이미지 정규화...\n",
    "\n",
    "from torch.utils.data import DataLoader  # 학습 및 배치로 모델에 넣어주기 위한 툴\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "import cv2   # 이미지 출력하고, 저장하고, 새 윈도우에 보여주고 등등\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    #필요한 변수 선언\n",
    "    def __init__(self,img_path_list,label_list,train_mode = True, transforms = None):\n",
    "        self.transforms = transforms\n",
    "        self.train_mode = train_mode\n",
    "        self.img_path_list = img_path_list\n",
    "        self.label_list = label_list\n",
    "    \n",
    "    # index번째 data를 return\n",
    "    def __getitem__(self,index):\n",
    "        img_path = self.img_path_list[index]  # 이미지 파일 경로\n",
    "        print(img_path)\n",
    "        \n",
    "        # Get image data\n",
    "        image = cv2.imread(img_path)  # 이미지 파일\n",
    "        if self.transforms is not None:   # transfrom 형태가 주어지면 transfrom하여 저장\n",
    "            image = self.transforms(image)\n",
    "            \n",
    "        if self.train_mode:\n",
    "            label = self.label_list[index]\n",
    "            return image, label\n",
    "        else:\n",
    "            return image\n",
    "        \n",
    "    def __len__(self):  # 길이 출력\n",
    "        return len(self.img_path_list)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f0118ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 임시 데이터셋 만들어 정상 작동하는지 확인\n",
    "tempdataset = CustomDataset(all_img_path, all_label,train_mode = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5ad03f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#plt.imshow(tempdataset.__getitem__(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28ed4df6",
   "metadata": {},
   "source": [
    "# Kernal Died ?? Prolly GPU 사용 못해서 메모리 사용량 과다로 죽은듯"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e369a788",
   "metadata": {},
   "source": [
    "## 3. Train / Validation Split\n",
    "\n",
    "학습시킬 데이터셋과 검증할 데이터셋 분리!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0379b35b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train : Validation = 0.75: 0.25 Split\n",
    "\n",
    "train_len = int(len(all_img_path)*0.75)\n",
    "Vali_len = int(len(all_img_path)*0.25)\n",
    "\n",
    "train_img_path = all_img_path[:train_len]\n",
    "train_label = all_label[:train_len]\n",
    "\n",
    "vali_img_path = all_img_path[train_len:]\n",
    "vali_label = all_label[train_len:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e9d0e823",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train set 길이: 542\n",
      "validation set 길이: 180\n"
     ]
    }
   ],
   "source": [
    "print(\"train set 길이:\", train_len)\n",
    "print(\"validation set 길이:\", Vali_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79f59075",
   "metadata": {},
   "source": [
    "## 4. Transfrom\n",
    "\n",
    "나뉜 데이터셋에서 이미지를 분석하기 위해 이미지 변형(transform) 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2ecb8c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),  # Numpy 배열에서 PIL 이미지로 변형\n",
    "    transforms.Resize([CFG['IMG_SIZE'],CFG['IMG_SIZE']]), # 이미지 사이즈 변형\n",
    "    transforms.ToTensor(), #이미지 데이터를 tensor\n",
    "    transforms.Normalize(mean= (0.5,0.5,0.5),std = (0.5,0.5,0.5)) # 이미지 정규화\n",
    "])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize([CFG['IMG_SIZE'], CFG['IMG_SIZE']]),\\\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean =(0.5,0.5,0.5),std = (0.5,0.5,0.5))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25f3bcda",
   "metadata": {},
   "source": [
    "## 5. Dataloader\n",
    "- `Dataloader` Class 생성: batch 기반 딥러닝모델 학습을 위해 mini batch를 만들어주는 역할\n",
    "\n",
    "- dataset의 전체 데이터가 batch size로 나뉨\n",
    "\n",
    "- 만들었던 dataset을 input으로 넣어주면 여러 옵션(데이터 묶기, 섞기, 알아서 병렬처리)를 통해 batch 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "429d00c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Dataloader\n",
    "\n",
    "# CustomDataset class를 통하여 train dataset 생성\n",
    "train_dataset = CustomDataset(train_img_path, \n",
    "                              train_label, \n",
    "                              train_mode = True, \n",
    "                              transforms = train_transform)\n",
    "# 만든 train dataset를 DataLoader에 넣어 batch 만들기\n",
    "train_loader = DataLoader(train_dataset,\n",
    "                          batch_size = CFG['BATCH_SIZE'],\n",
    "                          shuffle=True, \n",
    "                          num_workers =0)\n",
    "\n",
    "\n",
    "# Validation 에서도 적용\n",
    "vali_dataset = CustomDataset(vali_img_path, \n",
    "                             vali_label, \n",
    "                             train_mode = True, \n",
    "                             transforms = test_transform)\n",
    "vali_loader = DataLoader(vali_dataset,\n",
    "                         batch_size = CFG['BATCH_SIZE'],\n",
    "                         shuffle=True, \n",
    "                         num_workers =0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "00ec68ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total train imgs:  542 / total train batches(iterations) : 46\n",
      "total valid imgs:  180 / total valid batches(iterations) : 16\n"
     ]
    }
   ],
   "source": [
    "train_batches = len(train_loader)\n",
    "vali_batches = len(vali_loader)\n",
    "\n",
    "print('total train imgs: ',train_len,'/ total train batches(iterations) :',train_batches)\n",
    "print('total valid imgs: ',Vali_len,'/ total valid batches(iterations) :',vali_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5591219d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch.utils.data.dataloader.DataLoader object at 0x0000026948232580>\n"
     ]
    }
   ],
   "source": [
    "print(vali_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac01a753",
   "metadata": {},
   "source": [
    "**train 데이터의 경우, 542 장의 이미지를, 12개짜리 배치로 나누어 1번의 epoch를 돌기 위해 46개의 batch를 반복 학습해야한다.**\n",
    "\n",
    "- 배치 사이즈 : 12 \n",
    "- train - 46 묶음 / validation - 16 묶음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32b0360a",
   "metadata": {},
   "source": [
    "# III. 모델링\n",
    "## 1. 모델 구조 정의\n",
    "CNN모델을 이해하기 위한 기본 개념"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2348180",
   "metadata": {},
   "source": [
    "### - Neural Network (신경망)\n",
    "신경망은 다른 말로 인공 신경망이라고도 불린다.\n",
    "\n",
    "신경망이란 뇌 속 뉴런의 망형 구조를 닮은 다층형 구조의 컴퓨팅 모델이다.\n",
    "여기에서 서로 연결된 처리 소자, 일명 '뉴런'이라는 것이 있으며 생물학적 뉴런을 수학적으로 모델린한 것인데, 이들이 서로 협력하여 출력 함수를 도출한다.\n",
    "\n",
    "신경망은 입력 및 출력 계층/차원으로 구성되며 대부분은 숨겨진 계층으로 이루어져 있다. 숨겨진 계층은 입력을 출력 계층에서 사용할 수 있는 무언가로 변환해주는 단위로 구성된다. 따라서, 인공신경망 뉴런은 여러 입력값을 받아 일정 수준을 넘어서게 되면 활성화되고 출력값을 내보낸다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fe7a3be",
   "metadata": {},
   "source": [
    "### 1) Activation Function (활성화 함수)\n",
    "\n",
    "딥러닝 네트워크에서 노드에 입력된 값들을 비선형 함수에 통과시킨 후 다음 레이어로 전달하는 데, 이 떄 사용하는 함수를 활성화 함수(Activation Function)이라 한다. \n",
    "\n",
    "선형 함수가 아니라 비선형 함수를 사용하는 이유는 딥러닝 모델의 레이어 층을 깊게 가져갈 수 있기 때문이다.\n",
    "\n",
    "**대표적인 활성화 함수 살펴보기**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac58fc66",
   "metadata": {},
   "source": [
    "#### 1.1) Sigmoid 함수\n",
    "\n",
    "Sigmoid 함수는 Logistic 함수라고 불리기도 하며, x의 값에 따라 0~1의 값을 출력하는 S자형 함수이다.\n",
    "\n",
    "`Binary Classification` 마지막 레이어의 활성화 함수로 사용된다.\n",
    "\n",
    "Sigmoid 함수는 Activation function으로 많이 사용되며, 보통 0.5 미만은 0, 이상은 1을 출력한다.\n",
    "\n",
    "$$sigmoid(x)\\\\\n",
    "= 1 / (1 + exp(-x))\\\\\n",
    "= exp(x) / (exp(x) + 1)$$\n",
    "\n",
    "![시그모이드함수](https://upload.wikimedia.org/wikipedia/commons/thumb/8/88/Logistic-curve.svg/480px-Logistic-curve.svg.png)\n",
    "\n",
    "그래프를 보면, 양 극단 값의 기울기(미분 값)이 0에 가까워져, 학습이 되지 않는 문제가 발생한다.\n",
    "\n",
    "이를 **Gradient Vanishing 문제**라고 하는데 이러한 문제를 해결하기 위해 다양한 활성화함수들이 고안되었다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f1dd978",
   "metadata": {},
   "source": [
    "### 2) Perceptron (퍼셉트론)\n",
    "\n",
    "퍼셉트론이란 신경망의 기원이 되는 개념으로, Frank Rosenblatt가 1957년에 고안한 알고리즘이다. \n",
    "\n",
    "이는 여러 신호를 입력받아 일정한 임계값을 넘는지를 확인하여, 0(흐르지 않는다, 임계값을 넘지 않았다) 또는 1(흐른다, 임계값을 넘었다)라는 출력값을 앞으로 전달한다.\n",
    "\n",
    "**(뉴런의 all-or-none 늒김)**\n",
    "\n",
    "각 입력 신호 X는 각 가중치 w와 곱해진다. 가중치가 클수록 그 신호가 중요하다는 뜻이다.\n",
    "\n",
    "그러나, 퍼셉트론은 단순한 선형 분류기로, AND나 OR과 같은 분류는 가능하나, XOR 분류는 불가능하다.\n",
    "\n",
    "![퍼셉트론 러닝 알고리즘](https://image.slidesharecdn.com/machine-learning-120930145310-phpapp01/95/machine-learning-with-applications-in-categorization-popularity-and-sequence-labeling-75-638.jpg?cb=1354541953)\n",
    "\n",
    "\n",
    "**편향(bias) = $theta$ (임계치)** \n",
    "\n",
    ": 노드를 얼마나 쉽게 활성화(1로 출력; activation) 되느냐를 조정하는 매개변수\n",
    "* theta (임계치)가 높아질수록, 엄격한 모델이 되며, underfitting의 문제 발생 가능\n",
    "* theat (임계치)가 낮아질수록, 느슨한 모델이 되며, overfitting, 과도한 noise 포함 등의 문제 발생 \n",
    "\n",
    "\n",
    "[<링크> 개념에 대한 자세한 설명](https://velog.io/@skyepodium/%ED%8D%BC%EC%85%89%ED%8A%B8%EB%A1%A0%EC%9D%B4-%EC%99%9C-XOR-%EB%AC%B8%EC%A0%9C%EB%A5%BC-%EB%AA%BB%ED%91%B8%EB%8A%94%EC%A7%80-%EC%95%8C%EC%95%84%EB%B3%B4%EA%B8%B0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80c685ba",
   "metadata": {},
   "source": [
    "### 3) MLP (Multi-Layer Perceptron 다층 퍼셉트론)\n",
    "\n",
    "직선형(선형) 영역만 구분하고 표시할 수 있는 단층 퍼셉트론의 한계를 극복하기 위해 고안된 것이 다층 퍼셉트론이다.\n",
    "\n",
    "XOR 문제의 경우, 각 영역을 직선으로 분리하는게 불가능한데, 비선형 영역까지 표현 가능한 다층 퍼셉트론으로 XOR 게이트까지 구현할 수 있다.\n",
    "\n",
    "![다층 퍼셉트론 구조](https://velog.velcdn.com/images%2Fskyepodium%2Fpost%2F10424d38-94ea-4228-aeda-37757f7bfa99%2F%E1%84%89%E1%85%B3%E1%84%8F%E1%85%B3%E1%84%85%E1%85%B5%E1%86%AB%E1%84%89%E1%85%A3%E1%86%BA%202021-05-19%20%E1%84%8B%E1%85%A9%E1%84%92%E1%85%AE%201.11.02.png)\n",
    "\n",
    "![다층 퍼셉트론 XOR](https://velog.velcdn.com/images%2Fskyepodium%2Fpost%2F63930671-cb82-43a2-825b-e822beec415f%2F%E1%84%89%E1%85%B3%E1%84%8F%E1%85%B3%E1%84%85%E1%85%B5%E1%86%AB%E1%84%89%E1%85%A3%E1%86%BA%202021-05-19%20%E1%84%8B%E1%85%A9%E1%84%92%E1%85%AE%201.03.37.png)\n",
    "\n",
    "\n",
    "--------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6d316038",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "import torch.nn as nn # 신경망들이 포함됨\n",
    "#import torch.nn.init as init # 텐서에 초기값을 줌\n",
    "\n",
    "class CNNclassification(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNNclassification, self).__init__()\n",
    "        self.layer1 = torch.nn.Sequential(\n",
    "            nn.Conv2d(3, 8, kernel_size=3, stride=1, padding=1), #cnn layer\n",
    "            nn.ReLU(), #activation function\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)) #pooling layer\n",
    "        \n",
    "        self.layer2 = torch.nn.Sequential(\n",
    "            nn.Conv2d(8, 16, kernel_size=3, stride=1, padding=1), #cnn layer\n",
    "            nn.ReLU(), #activation function\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)) #pooling layer\n",
    "        \n",
    "        self.layer3 = torch.nn.Sequential(\n",
    "            nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1), #cnn layer\n",
    "            nn.ReLU(), #activation function\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)) #pooling layer\n",
    "        \n",
    "        self.layer4 = torch.nn.Sequential(\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=1, padding=1), #cnn layer\n",
    "            nn.ReLU(), #activation function\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)) #pooling layer\n",
    "        \n",
    "        self.fc_layer = nn.Sequential( \n",
    "            nn.Linear(3136, 10) #fully connected layer(ouput layer)\n",
    "        )    \n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.layer1(x) #1층\n",
    "        \n",
    "        x = self.layer2(x) #2층\n",
    "         \n",
    "        x = self.layer3(x) #3층\n",
    "        \n",
    "        x = self.layer4(x) #4층\n",
    "        \n",
    "        x = torch.flatten(x, start_dim=1) # N차원 배열 -> 1차원 배열\n",
    "        \n",
    "        out = self.fc_layer(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af64ae78",
   "metadata": {},
   "source": [
    "## Convolution Layer\n",
    "\n",
    "Convolution Layer에서는 이미지의 특징(feature map)을 추출해낸다.\n",
    "\n",
    "입력 데이터가 주어지면 필터를 이용해 특징을 추출한 다음 아웃풋을 내보낸다.\n",
    "\n",
    "이 필터는 `커널(kernel)` 혹은 `가중치의 배열`이라고도 부르며, 이 값을 조정하는 것이 곧 학습을 의미한다.\n",
    "\n",
    "![convolution layer](https://cdn-images-1.medium.com/max/1200/1*1okwhewf5KCtIPaFib4XaA.gif)\n",
    "\n",
    "**Blue: Input / Green: Output**\n",
    "\n",
    "**2D convolution using a kernel size of 3, stride of 1 and padding**\n",
    "\n",
    "\n",
    "![convolution input_channel output_channel example](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FbptGfh%2FbtrxiFKPKnF%2FvxsCM7XDcvFHBoWbWFKpQK%2Fimg.png)\n",
    "\n",
    "\n",
    "### Kernel size\n",
    "\n",
    "Convolution의 시야(view)를 결정한다. 보통 2D에서 3x3 pixel로 사용한다.\n",
    "\n",
    "첫번째 Convolution Layer에서도 3x3 크기의 커널을 사용했다. 이는 곧 학습해야할 가중치가 9개라는 뜻이다. \n",
    "\n",
    "\n",
    "### Stride\n",
    "\n",
    "이미지를 횡단할 때 커널의 스텝사이즈를 결정한다. 즉, 커널을 이동시키는 거리이다.\n",
    "\n",
    "기본값을 1이지만, 보통 max pooling과 비슷하게 이미지를 다운샘플링하기 위해 stride를 2로 사용하기도 한다.\n",
    "\n",
    "Covolution을 하게되면 입력 데이터의 크기가 자연스럽게 줄어들게 되는데, 주변에 값이 0인 셀들을 추가(Padding)해서 입력 데이터의 크기를 유지시키기도 한다.\n",
    "\n",
    "\n",
    "### Padding\n",
    "\n",
    "샘플의 테두리를 어떻게 조절할 지를 결정한다. \n",
    "\n",
    "Zero padding은 이미지 주위에 0을 둘러서, layer를 통과할 때 이미지 데이터의 축소를 방지해주는 역할을 한다. \n",
    "\n",
    "패딩된 convolution은 input과 동일한 output 차원을 유지하는 반면, 패딩되지 않은 convolution은 커널이 1보다 큰 경우 테두리의 일부를 잘라버릴 수 있다. \n",
    "\n",
    "\n",
    "### ReLU\n",
    "\n",
    "활성화 함수 중 하나이다. \n",
    "\n",
    "Gradient vanishing 문제를 해결 가능하고 계산이 빠르고 양 극단값이 포화되지 않는다는 장점이 있다.\n",
    "\n",
    "\n",
    "### Pooling Layer ⊃ Max Pooling\n",
    "\n",
    "Pooling layer는 데이터의 공간적 크기를 축소하는 데 사용한다.\n",
    "\n",
    "보통 이 레이어에서 이미지의 크기를 조절하며, CNN에서는 주로 Max-pooling 방식을 사용한다. \n",
    "\n",
    "Conv layer은 이미지의 특정 영역의 특징을 잡아내는 역할이라면, Pooling은 이미지의 크기를 줄이는 동시에 이미지 전체를 볼 수 있게 도와준다.\n",
    "\n",
    "Max pooling에서는 선택된 영역에서 가장 큰 값을 뽑아 대표값으로 설정한다.\n",
    "\n",
    "이를 통해 **학습 시간을 단축**하고 **오버피팅 문제**를 완화할 수 있다.\n",
    "\n",
    "\n",
    "### Fully Connected (output) Layer\n",
    "\n",
    "이전 레이어의 출력을 평탄화(flatten)하여 다음 스테이지의 입력이 될 수 있는 **단일 벡터로 변환**한다.\n",
    "\n",
    "마지막으로 각 라벨에 대한 **최종 확률**을 제공한다.\n",
    "\n",
    "\n",
    "### Input & Output channels\n",
    "\n",
    "Convolution layer는 input 채널의 특정 수($I$)를 받아 output 채널의 특정 수($O$)를 계산한다. \n",
    "\n",
    "K를 커널의 수라고 할 때, 이런 계층에서 필요한 파라미터의 수는 $I \\times O \\times K$로 계산할 수 있다. \n",
    "\n",
    "\n",
    "![CNN 전과정](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FDSJmz%2FbtrxiXFh1M2%2FK1LX9HCz4dEskKOK9WT4Vk%2Fimg.png)\n",
    "\n",
    "![CNN 전과정 2](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FbasjDA%2Fbtrxr497kPE%2FxjyADtaFK3LPI6TAqBdaK1%2Fimg.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0891258",
   "metadata": {},
   "source": [
    "## 2. 모델 학습\n",
    "\n",
    "모델 학습을 하기위해 매개변수 정의하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "22f7b357",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim  #최적화 알고리즘들을 포함하는 패키지\n",
    "\n",
    "model = CNNclassification().to(device)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(params = model.parameters(),lr = CFG['LEARNING_RATE'])\n",
    "scheduler = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b0fe9b9",
   "metadata": {},
   "source": [
    "### Classification model\n",
    "\n",
    "이번 베이스라인 프로젝트 모델에서는 기본 **`CNN classification 모델`** 사용했다.\n",
    "\n",
    "### Loss function\n",
    "\n",
    "손실함수로는 classification 문제이기 때문에 **`CrossEntropyLoss`**를 사용했다.\n",
    "\n",
    "손실함수는 실제 값과 모델이 예측한 값의 거리를 출력하는 함수로, 예측이 얼마나 틀렸는지 알려주는 함수이다.\n",
    "\n",
    "\"모델의 예측이 얼마나 틀렸는지\"를 어떻게 정의하느냐에 따라 어떤 Loss function을 사용할 지가 정해진다. \n",
    "\n",
    "\n",
    "### Optimizer\n",
    "\n",
    "최적화 함수로는 확률적 경사 하강법 **`SGD(Stochastic Gradient Descent)`**를 사용했다.\n",
    "\n",
    "Optimizer는 학습 데이터(Train data)를 이용하여 모델을 학습할 때 데이터의 실제 결과와 오델이 예측한 결과를 기반으로 오차를 잘 줄일 수 있게 만들어주는 역할을 한다.\n",
    "\n",
    "여기서 Learning rate, 학습률은 얼마나 빠른 속도로 이동하는 지를 결정한다.\n",
    "\n",
    "Learning rate를 엄청 크게 설정한다면 원하는 값까지 빠르게 도달할 수 있지만, 자칫하면 오히려 최소값에 수렴하지 못할 수도 있다.\n",
    "\n",
    "반면 너무 작은 경우에는 최소값에 도달할 수 있을 지는 몰라도, 시간이 매우 오래 걸리고, overfitting의 문제가 발생할 수 있다.\n",
    "\n",
    "따라서 적절한 learning rate를 설정하는 과정이 중요하다.\n",
    "\n",
    "이제 train 함수를 만들고 train 데이터를 학습시켜 validation으로 평가하는 메소드를 작성해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "712f4de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm  # 반복문의 어디까지 반복이 진행되었는지 표시!!\n",
    "\n",
    "def train(model, optimizer, train_loader, scheduler, device):\n",
    "    model.to(device)\n",
    "    n = len(train_loader)\n",
    "    best_acc = 0  # 0으로 초기화\n",
    "    \n",
    "    for epoch in range(1, CFG[\"EPOCHS\"]+1):  # 에포크 설정\n",
    "        model.train()  #모델 학습\n",
    "        running_loss = 0.0  # 0으로 초기화\n",
    "        \n",
    "        for img,label in tqdm(iter(train_loader)):\n",
    "            img,label = img.to(device),label.to(device)  # 배치 데이터\n",
    "            optimizer.zero_grad()  # 배치마다 optimizer 초기화\n",
    "            \n",
    "            # Data -> Model -> Output\n",
    "            logit = model(img)  # 예측값 산출, logit : 최종값, 스코어\n",
    "            loss = criterion(logit, label)  # `criterion()` : 손실함수 계산 (앞에서 crossentropy로 정함)\n",
    "            \n",
    "            # Back Propagation 역전파\n",
    "            loss.backward()  # 손실함수 기준 역전파\n",
    "            optimizer.step()  # 가중치 업데이트\n",
    "            running_loss += loss.item()\n",
    "            \n",
    "            print(running_loss)\n",
    "        \n",
    "        print('[%d] Train loss: %.10f' %(epoch, running_loss/len(train_loader)))\n",
    "        \n",
    "        if scheduler is not None:\n",
    "            scheduler.step()   # 하이퍼파라미터 업데이트\n",
    "            \n",
    "            \n",
    "            \n",
    "        # ***************Validation set 평가****************\n",
    "        \n",
    "        model.eval()  # evaluation 과정에서 사용하지 않아야 하는 layer들을 알아서 off 시킴\n",
    "        vali_loss = 0.0\n",
    "        correct = 0\n",
    "        with torch.no_grad():  # 파라미터 업데이트 안하기 때문에 no_grad 사용  \n",
    "            for img,label in tqdm(iter(vali_loader)):\n",
    "                img,label = img.to(device), label.to(device)\n",
    "                \n",
    "                logit = model(img)\n",
    "                vali_loss += criterion(logit,label)\n",
    "                pred = logit.argmax(dim = 1, keepdim =True) # 11개의 class 중 가장 값이 높은 것을 예측 label로 추출\n",
    "                correct += pred.eq(label.view_as(pred)).sum().item()  # 에측값과 실제값이 맞으면 1, 아니면 0\n",
    "        vali_acc = 100 * correct / len(vali_loader.dataset)\n",
    "        print('[Vali set] Avg Loss: {:.4f}, Accuracy: {}/{} ( {:.0f}%)\\n'.format(vali_loss / len(vali_loader),correct, len(vali_loader.dataset), 100 * correct/ len(vali_loader.dataset)))\n",
    "\n",
    "        \n",
    "        # 베스트 모델 저장\n",
    "        if best_acc < vali_acc:\n",
    "            best_acc = vali_acc\n",
    "            torch.save(model.state_dict(),'best_model.pth') # 이 디렉토리에 best_model.pth를 저장\n",
    "            print('Model Saved.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9aecebfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for img,label in  tqdm(iter(train_loader)):\n",
    "    #print(\"************\",i,\"************\")\n",
    "    #print(\"img:\",img)\n",
    "    #print(\"label:\",label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c5a0eda",
   "metadata": {},
   "source": [
    "### Logit \n",
    "\n",
    "확률화되지 않은 예측 결과를 logit이라고 한다. 확률값을 계산하지 않고 layer를 거쳐 나온 산출물을 그대로 다음 layer에 넘길 때 사용한다.\n",
    "\n",
    "즉, logit은 신경망의 최종 레이어가 내놀은 확률 값이 아닌 중간 결과물이고, 결과값의 범위가 실수 전체이다. \n",
    "\n",
    "![logit](https://velog.velcdn.com/images%2Fguide333%2Fpost%2Feb996ddf-1ff4-48e7-b8f9-fa23b622505a%2FScreenshot%20from%202021-05-17%2011-01-37.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edb1647d",
   "metadata": {},
   "source": [
    "Classification 문제이기 때문에 평가지표로는 Accuracy를 사용하여 모델의 정확도를 산출하였다.\n",
    "\n",
    "\n",
    "### Epoch\n",
    "\n",
    "딥러닝에서 epoch는 전체 트레이닝 셋이 신경망을 통과한 횟수이다.\n",
    "\n",
    "1-epoch는 전체 트레이닝 셋이 하나의 신경망에 적용되어 순전파와 역전파를 통해 한번 통과했다는 뜻이다.\n",
    "\n",
    "Epoch가 많다고 꼭 학습이 잘 되는 것은 아니다. \n",
    "Epoch가 너무 적을 경우 학습이 덜 이루어질 수 있고, 너무 많을 경우 과적합이 되는 경우도 있다. 따라서, 적절한 epoch를 설정하는 게 중요하다.\n",
    "\n",
    "이때 validation loss와 accuracy는 epoch를 언제 중단 할 지 모니터링하는 용도로 사용되기도 한다.\n",
    "\n",
    "\n",
    "### Batch size\n",
    "\n",
    "Batch size란 CPU 또는 GPU 연산 시, 하드웨어로 한번에 로드되는 데이터의 개수이다. \n",
    "\n",
    "개인 컴퓨터 환경(메모리 용량)에 따라 batch size를 조절해야하며, 이는 모델 학습 과정에 영향을 끼치기도 하기 때문에, 적절히 설정하는 것이 중요하다.\n",
    "\n",
    "\n",
    "### Backpropagation (역전파)\n",
    "\n",
    "오차 역전파법이라고도 하며 예측값과 실제값의 차이인 오차를 계산하고고, 이것을 다시 역으로 input 단에 보내, 오차가 작아지는 방향으로 가중치를 수정하는 과정을 일정 횟수 반복하는 방법이다.\n",
    "\n",
    "이때 역전파 과정에서는 앞서 언급했던 `최적화 함수`를 이용한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1c297eac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                           | 0/46 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset/train/204.PNG\n",
      "dataset/train/435.PNG\n",
      "dataset/train/341.PNG\n",
      "dataset/train/436.PNG\n",
      "dataset/train/121.PNG\n",
      "dataset/train/398.PNG\n",
      "dataset/train/170.PNG\n",
      "dataset/train/375.PNG\n",
      "dataset/train/106.PNG\n",
      "dataset/train/130.PNG\n",
      "dataset/train/213.PNG\n",
      "dataset/train/187.PNG\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  2%|█▊                                                                                 | 1/46 [00:00<00:26,  1.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.305204153060913\n",
      "dataset/train/450.PNG\n",
      "dataset/train/039.PNG\n",
      "dataset/train/522.PNG\n",
      "dataset/train/528.PNG\n",
      "dataset/train/048.PNG\n",
      "dataset/train/465.PNG\n",
      "dataset/train/166.PNG\n",
      "dataset/train/088.PNG\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  4%|███▌                                                                               | 2/46 [00:01<00:22,  1.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset/train/113.PNG\n",
      "dataset/train/307.PNG\n",
      "dataset/train/334.PNG\n",
      "dataset/train/536.PNG\n",
      "4.597833633422852\n",
      "dataset/train/519.PNG\n",
      "dataset/train/346.PNG\n",
      "dataset/train/178.PNG\n",
      "dataset/train/417.PNG\n",
      "dataset/train/444.PNG\n",
      "dataset/train/160.PNG\n",
      "dataset/train/253.PNG\n",
      "dataset/train/252.PNG\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  7%|█████▍                                                                             | 3/46 [00:01<00:19,  2.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset/train/428.PNG\n",
      "dataset/train/061.PNG\n",
      "dataset/train/006.PNG\n",
      "dataset/train/366.PNG\n",
      "6.892958879470825\n",
      "dataset/train/236.PNG\n",
      "dataset/train/030.PNG\n",
      "dataset/train/456.PNG\n",
      "dataset/train/202.PNG\n",
      "dataset/train/117.PNG\n",
      "dataset/train/350.PNG\n",
      "dataset/train/214.PNG\n",
      "dataset/train/293.PNG\n",
      "dataset/train/463.PNG\n",
      "dataset/train/145.PNG\n",
      "dataset/train/338.PNG\n",
      "dataset/train/085.PNG\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  9%|███████▏                                                                           | 4/46 [00:01<00:19,  2.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.190081596374512\n",
      "dataset/train/316.PNG\n",
      "dataset/train/411.PNG\n",
      "dataset/train/102.PNG\n",
      "dataset/train/303.PNG\n",
      "dataset/train/330.PNG\n",
      "dataset/train/447.PNG\n",
      "dataset/train/086.PNG\n",
      "dataset/train/481.PNG\n",
      "dataset/train/081.PNG\n",
      "dataset/train/507.PNG\n",
      "dataset/train/407.PNG\n",
      "dataset/train/066.PNG\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 11%|█████████                                                                          | 5/46 [00:02<00:19,  2.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.498095750808716\n",
      "dataset/train/440.PNG\n",
      "dataset/train/409.PNG\n",
      "dataset/train/266.PNG\n",
      "dataset/train/165.PNG\n",
      "dataset/train/068.PNG\n",
      "dataset/train/367.PNG\n",
      "dataset/train/193.PNG\n",
      "dataset/train/058.PNG\n",
      "dataset/train/275.PNG\n",
      "dataset/train/466.PNG\n",
      "dataset/train/075.PNG\n",
      "dataset/train/072.PNG\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 13%|██████████▊                                                                        | 6/46 [00:02<00:18,  2.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13.808849334716797\n",
      "dataset/train/429.PNG\n",
      "dataset/train/512.PNG\n",
      "dataset/train/222.PNG\n",
      "dataset/train/174.PNG\n",
      "dataset/train/242.PNG\n",
      "dataset/train/331.PNG\n",
      "dataset/train/035.PNG\n",
      "dataset/train/091.PNG\n",
      "dataset/train/037.PNG\n",
      "dataset/train/283.PNG\n",
      "dataset/train/034.PNG\n",
      "dataset/train/235.PNG\n",
      "16.120121002197266\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 15%|████████████▋                                                                      | 7/46 [00:03<00:17,  2.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset/train/295.PNG\n",
      "dataset/train/217.PNG\n",
      "dataset/train/480.PNG\n",
      "dataset/train/203.PNG\n",
      "dataset/train/239.PNG\n",
      "dataset/train/177.PNG\n",
      "dataset/train/200.PNG\n",
      "dataset/train/394.PNG\n",
      "dataset/train/128.PNG\n",
      "dataset/train/305.PNG\n",
      "dataset/train/460.PNG\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 17%|██████████████▍                                                                    | 8/46 [00:04<00:25,  1.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset/train/248.PNG\n",
      "18.416425466537476\n",
      "dataset/train/493.PNG\n",
      "dataset/train/230.PNG\n",
      "dataset/train/392.PNG\n",
      "dataset/train/008.PNG\n",
      "dataset/train/495.PNG\n",
      "dataset/train/033.PNG\n",
      "dataset/train/184.PNG\n",
      "dataset/train/046.PNG\n",
      "dataset/train/541.PNG\n",
      "dataset/train/109.PNG\n",
      "dataset/train/201.PNG\n",
      "dataset/train/323.PNG\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 20%|████████████████▏                                                                  | 9/46 [00:04<00:21,  1.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20.732285022735596\n",
      "dataset/train/500.PNG\n",
      "dataset/train/525.PNG\n",
      "dataset/train/096.PNG\n",
      "dataset/train/001.PNG\n",
      "dataset/train/269.PNG\n",
      "dataset/train/051.PNG\n",
      "dataset/train/390.PNG\n",
      "dataset/train/388.PNG\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 22%|█████████████████▊                                                                | 10/46 [00:05<00:18,  1.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset/train/534.PNG\n",
      "dataset/train/097.PNG\n",
      "dataset/train/125.PNG\n",
      "dataset/train/524.PNG\n",
      "23.013492345809937\n",
      "dataset/train/132.PNG\n",
      "dataset/train/279.PNG\n",
      "dataset/train/139.PNG\n",
      "dataset/train/111.PNG\n",
      "dataset/train/434.PNG\n",
      "dataset/train/027.PNG\n",
      "dataset/train/254.PNG\n",
      "dataset/train/418.PNG\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 24%|███████████████████▌                                                              | 11/46 [00:05<00:16,  2.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset/train/441.PNG\n",
      "dataset/train/403.PNG\n",
      "dataset/train/302.PNG\n",
      "dataset/train/009.PNG\n",
      "25.33716917037964\n",
      "dataset/train/078.PNG\n",
      "dataset/train/104.PNG\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 24%|███████████████████▌                                                              | 11/46 [00:05<00:18,  1.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset/train/457.PNG\n",
      "dataset/train/259.PNG\n",
      "dataset/train/420.PNG\n",
      "dataset/train/012.PNG\n",
      "dataset/train/408.PNG\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [29]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [27]\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, optimizer, train_loader, scheduler, device)\u001b[0m\n\u001b[0;32m      9\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()  \u001b[38;5;66;03m#모델 학습\u001b[39;00m\n\u001b[0;32m     10\u001b[0m running_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m  \u001b[38;5;66;03m# 0으로 초기화\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m img,label \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28miter\u001b[39m(train_loader)):\n\u001b[0;32m     13\u001b[0m     img,label \u001b[38;5;241m=\u001b[39m img\u001b[38;5;241m.\u001b[39mto(device),label\u001b[38;5;241m.\u001b[39mto(device)  \u001b[38;5;66;03m# 배치 데이터\u001b[39;00m\n\u001b[0;32m     14\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()  \u001b[38;5;66;03m# 배치마다 optimizer 초기화\u001b[39;00m\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tqdm\\std.py:1195\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1192\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[0;32m   1194\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1195\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[0;32m   1196\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[0;32m   1197\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[0;32m   1198\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:681\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    678\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    679\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    680\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 681\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    682\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    683\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    684\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    685\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:721\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    719\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    720\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 721\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    722\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    723\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:49\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfetch\u001b[39m(\u001b[38;5;28mself\u001b[39m, possibly_batched_index):\n\u001b[0;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[1;32m---> 49\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     51\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:49\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfetch\u001b[39m(\u001b[38;5;28mself\u001b[39m, possibly_batched_index):\n\u001b[0;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[1;32m---> 49\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     51\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Input \u001b[1;32mIn [16]\u001b[0m, in \u001b[0;36mCustomDataset.__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28mprint\u001b[39m(img_path)\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# Get image data\u001b[39;00m\n\u001b[1;32m---> 23\u001b[0m image \u001b[38;5;241m=\u001b[39m \u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg_path\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# 이미지 파일\u001b[39;00m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:   \u001b[38;5;66;03m# transfrom 형태가 주어지면 transfrom하여 저장\u001b[39;00m\n\u001b[0;32m     25\u001b[0m     image \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms(image)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train(model, optimizer, train_loader, scheduler, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05c3f92d",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a22ad6e",
   "metadata": {},
   "source": [
    "[Vali set] Avg Loss: 0.9572, Accuracy: 140/181 ( 77%)\n",
    "\n",
    "Model Saved."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaee8f33",
   "metadata": {},
   "source": [
    "**Epoch가 [   ]일 때 Validation Accuracy가 [  ]%로 best_model에 선정되어 저장되었다.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34216eb9",
   "metadata": {},
   "source": [
    "## 3. 추론하기 (Inference) - Test 데이터에 적용하기\n",
    "\n",
    "이제 학습된 best_model을 가지고 test 셋의 라벨을 추론해보도록 하자.\n",
    "\n",
    "`with torch.no_grad():`\n",
    "\n",
    ": Autograd 엔진을 꺼버려 더이상 자동으로 gradient 트래킹을 하지 않는다. 이렇게 함으로써 메모리 사용량을 줄이고 연산 속도를 높히기 위함이다. \n",
    " \n",
    " <br/>\n",
    "\n",
    "`model.eval()`\n",
    "\n",
    ": train 데이터를 활용하는 모델링 단계와 test 데이터로 추론하는 단계에서 다르게 동작하는 layer들이 존재한다. 예를 들면, Dropout layer나 BatchNorm 같은 레이어는 학습 시에만 동작한다. \n",
    "\n",
    "이런 layer들의 동작을 inference(evaluation) mode로 바꿔주는 목적으로 사용된다. \n",
    "\n",
    "**따라서, inference를 진행할 때에는 두가지 메소드를 다 호출해야한다!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "99b5aef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model,test_loader,device):\n",
    "    model.eval()  # evaluation 과정에서 사용하지 않아야 하는 layer들을 알아서 off 시킴\n",
    "    model_pred = []\n",
    "    with torch.no_grad():\n",
    "        for img in tqdm(iter(test_loader)):\n",
    "            img = img.to(device)\n",
    "            \n",
    "            pred_logit = model(img)\n",
    "            pred_logit = pred_logit.argmax(dim =1, keepdim = True).squeeze(1)\n",
    "            \n",
    "            model_pred.extend(pred_logit.tolist())\n",
    "    return model_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "16506950",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                           | 0/17 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset/test/001.PNG\n",
      "dataset/test/002.PNG\n",
      "dataset/test/003.PNG\n",
      "dataset/test/004.PNG\n",
      "dataset/test/005.PNG\n",
      "dataset/test/006.PNG\n",
      "dataset/test/007.PNG\n",
      "dataset/test/008.PNG\n",
      "dataset/test/009.PNG\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  6%|████▉                                                                              | 1/17 [00:00<00:05,  3.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset/test/010.PNG\n",
      "dataset/test/011.PNG\n",
      "dataset/test/012.PNG\n",
      "dataset/test/013.PNG\n",
      "dataset/test/014.PNG\n",
      "dataset/test/015.PNG\n",
      "dataset/test/016.PNG\n",
      "dataset/test/017.PNG\n",
      "dataset/test/018.PNG\n",
      "dataset/test/019.PNG\n",
      "dataset/test/020.PNG\n",
      "dataset/test/021.PNG\n",
      "dataset/test/022.PNG\n",
      "dataset/test/023.PNG\n",
      "dataset/test/024.PNG\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 12%|█████████▊                                                                         | 2/17 [00:00<00:05,  2.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset/test/025.PNG\n",
      "dataset/test/026.PNG\n",
      "dataset/test/027.PNG\n",
      "dataset/test/028.PNG\n",
      "dataset/test/029.PNG\n",
      "dataset/test/030.PNG\n",
      "dataset/test/031.PNG\n",
      "dataset/test/032.PNG\n",
      "dataset/test/033.PNG\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 18%|██████████████▋                                                                    | 3/17 [00:01<00:04,  2.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset/test/034.PNG\n",
      "dataset/test/035.PNG\n",
      "dataset/test/036.PNG\n",
      "dataset/test/037.PNG\n",
      "dataset/test/038.PNG\n",
      "dataset/test/039.PNG\n",
      "dataset/test/040.PNG\n",
      "dataset/test/041.PNG\n",
      "dataset/test/042.PNG\n",
      "dataset/test/043.PNG\n",
      "dataset/test/044.PNG\n",
      "dataset/test/045.PNG\n",
      "dataset/test/046.PNG\n",
      "dataset/test/047.PNG\n",
      "dataset/test/048.PNG\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 24%|███████████████████▌                                                               | 4/17 [00:01<00:04,  2.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset/test/049.PNG\n",
      "dataset/test/050.PNG\n",
      "dataset/test/051.PNG\n",
      "dataset/test/052.PNG\n",
      "dataset/test/053.PNG\n",
      "dataset/test/054.PNG\n",
      "dataset/test/055.PNG\n",
      "dataset/test/056.PNG\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 29%|████████████████████████▍                                                          | 5/17 [00:01<00:04,  2.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset/test/057.PNG\n",
      "dataset/test/058.PNG\n",
      "dataset/test/059.PNG\n",
      "dataset/test/060.PNG\n",
      "dataset/test/061.PNG\n",
      "dataset/test/062.PNG\n",
      "dataset/test/063.PNG\n",
      "dataset/test/064.PNG\n",
      "dataset/test/065.PNG\n",
      "dataset/test/066.PNG\n",
      "dataset/test/067.PNG\n",
      "dataset/test/068.PNG\n",
      "dataset/test/069.PNG\n",
      "dataset/test/070.PNG\n",
      "dataset/test/071.PNG\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 35%|█████████████████████████████▎                                                     | 6/17 [00:02<00:03,  2.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset/test/072.PNG\n",
      "dataset/test/073.PNG\n",
      "dataset/test/074.PNG\n",
      "dataset/test/075.PNG\n",
      "dataset/test/076.PNG\n",
      "dataset/test/077.PNG\n",
      "dataset/test/078.PNG\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 41%|██████████████████████████████████▏                                                | 7/17 [00:02<00:03,  2.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset/test/079.PNG\n",
      "dataset/test/080.PNG\n",
      "dataset/test/081.PNG\n",
      "dataset/test/082.PNG\n",
      "dataset/test/083.PNG\n",
      "dataset/test/084.PNG\n",
      "dataset/test/085.PNG\n",
      "dataset/test/086.PNG\n",
      "dataset/test/087.PNG\n",
      "dataset/test/088.PNG\n",
      "dataset/test/089.PNG\n",
      "dataset/test/090.PNG\n",
      "dataset/test/091.PNG\n",
      "dataset/test/092.PNG\n",
      "dataset/test/093.PNG\n",
      "dataset/test/094.PNG\n",
      "dataset/test/095.PNG\n",
      "dataset/test/096.PNG\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 47%|███████████████████████████████████████                                            | 8/17 [00:02<00:03,  2.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset/test/097.PNG\n",
      "dataset/test/098.PNG\n",
      "dataset/test/099.PNG\n",
      "dataset/test/100.PNG\n",
      "dataset/test/101.PNG\n",
      "dataset/test/102.PNG\n",
      "dataset/test/103.PNG\n",
      "dataset/test/104.PNG\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 53%|███████████████████████████████████████████▉                                       | 9/17 [00:03<00:03,  2.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset/test/105.PNG\n",
      "dataset/test/106.PNG\n",
      "dataset/test/107.PNG\n",
      "dataset/test/108.PNG\n",
      "dataset/test/109.PNG\n",
      "dataset/test/110.PNG\n",
      "dataset/test/111.PNG\n",
      "dataset/test/112.PNG\n",
      "dataset/test/113.PNG\n",
      "dataset/test/114.PNG\n",
      "dataset/test/115.PNG\n",
      "dataset/test/116.PNG\n",
      "dataset/test/117.PNG\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 59%|████████████████████████████████████████████████▏                                 | 10/17 [00:03<00:02,  2.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset/test/118.PNG\n",
      "dataset/test/119.PNG\n",
      "dataset/test/120.PNG\n",
      "dataset/test/121.PNG\n",
      "dataset/test/122.PNG\n",
      "dataset/test/123.PNG\n",
      "dataset/test/124.PNG\n",
      "dataset/test/125.PNG\n",
      "dataset/test/126.PNG\n",
      "dataset/test/127.PNG\n",
      "dataset/test/128.PNG\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 65%|█████████████████████████████████████████████████████                             | 11/17 [00:04<00:02,  2.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset/test/129.PNG\n",
      "dataset/test/130.PNG\n",
      "dataset/test/131.PNG\n",
      "dataset/test/132.PNG\n",
      "dataset/test/133.PNG\n",
      "dataset/test/134.PNG\n",
      "dataset/test/135.PNG\n",
      "dataset/test/136.PNG\n",
      "dataset/test/137.PNG\n",
      "dataset/test/138.PNG\n",
      "dataset/test/139.PNG\n",
      "dataset/test/140.PNG\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 71%|█████████████████████████████████████████████████████████▉                        | 12/17 [00:04<00:01,  2.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset/test/141.PNG\n",
      "dataset/test/142.PNG\n",
      "dataset/test/143.PNG\n",
      "dataset/test/144.PNG\n",
      "dataset/test/145.PNG\n",
      "dataset/test/146.PNG\n",
      "dataset/test/147.PNG\n",
      "dataset/test/148.PNG\n",
      "dataset/test/149.PNG\n",
      "dataset/test/150.PNG\n",
      "dataset/test/151.PNG\n",
      "dataset/test/152.PNG\n",
      "dataset/test/153.PNG\n",
      "dataset/test/154.PNG\n",
      "dataset/test/155.PNG\n",
      "dataset/test/156.PNG\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|█████████████████████████████████████████████████████████▉                        | 12/17 [00:04<00:02,  2.49it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [31]\u001b[0m, in \u001b[0;36m<cell line: 19>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     16\u001b[0m model\u001b[38;5;241m.\u001b[39mload_state_dict(checkpoint)\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# Inference\u001b[39;00m\n\u001b[1;32m---> 19\u001b[0m preds \u001b[38;5;241m=\u001b[39m \u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     20\u001b[0m preds[\u001b[38;5;241m0\u001b[39m:\u001b[38;5;241m5\u001b[39m]\n",
      "Input \u001b[1;32mIn [30]\u001b[0m, in \u001b[0;36mpredict\u001b[1;34m(model, test_loader, device)\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m img \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28miter\u001b[39m(test_loader)):\n\u001b[0;32m      6\u001b[0m     img \u001b[38;5;241m=\u001b[39m img\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m----> 8\u001b[0m     pred_logit \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m     pred_logit \u001b[38;5;241m=\u001b[39m pred_logit\u001b[38;5;241m.\u001b[39margmax(dim \u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, keepdim \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m)\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     11\u001b[0m     model_pred\u001b[38;5;241m.\u001b[39mextend(pred_logit\u001b[38;5;241m.\u001b[39mtolist())\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Input \u001b[1;32mIn [25]\u001b[0m, in \u001b[0;36mCNNclassification.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m     34\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer1(x) \u001b[38;5;66;03m#1층\u001b[39;00m\n\u001b[1;32m---> 36\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m#2층\u001b[39;00m\n\u001b[0;32m     38\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer3(x) \u001b[38;5;66;03m#3층\u001b[39;00m\n\u001b[0;32m     40\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer4(x) \u001b[38;5;66;03m#4층\u001b[39;00m\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\container.py:139\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    137\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    138\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 139\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    140\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\activation.py:98\u001b[0m, in \u001b[0;36mReLU.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     97\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m---> 98\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrelu\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1457\u001b[0m, in \u001b[0;36mrelu\u001b[1;34m(input, inplace)\u001b[0m\n\u001b[0;32m   1455\u001b[0m     result \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrelu_(\u001b[38;5;28minput\u001b[39m)\n\u001b[0;32m   1456\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1457\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrelu\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1458\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "test_dataset = CustomDataset(test_img_path,\n",
    "                             None,\n",
    "                             train_mode = False,\n",
    "                             transforms= test_transform\n",
    "                            )\n",
    "test_loader = DataLoader(test_dataset,\n",
    "                        batch_size = CFG['BATCH_SIZE'],\n",
    "                         shuffle = False,\n",
    "                         num_workers = 0\n",
    "                        )\n",
    "\n",
    "\n",
    "# Validation Accuracy가 가장 뛰|어난 모델을 불러온다\n",
    "checkpoint = torch.load('best_model.pth')\n",
    "model = CNNclassification().to(device)\n",
    "model.load_state_dict(checkpoint)\n",
    "\n",
    "# Inference\n",
    "preds = predict(model, test_loader, device)\n",
    "preds[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "325ef014",
   "metadata": {},
   "source": [
    "# IV. 데이콘 제출하기\n",
    "\n",
    "## 1. Submission 파일 생성\n",
    "\n",
    "이제 예측한 값을 sample_submission.csv에 넣어 제출용 파일을 생성하자.\n",
    "제출한 뒤 리더보드를 통해 결과 확인."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9a2c145e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'preds' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [32]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m submission  \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdataset/sample_submission.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m submission[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mpreds\u001b[49m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'preds' is not defined"
     ]
    }
   ],
   "source": [
    "submission  = pd.read_csv('dataset/sample_submission.csv')\n",
    "submission['label'] = preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "d888c6bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_name</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>001.PNG</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>002.PNG</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>003.PNG</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>004.PNG</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>005.PNG</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  file_name  label\n",
       "0   001.PNG      7\n",
       "1   002.PNG      1\n",
       "2   003.PNG      9\n",
       "3   004.PNG      7\n",
       "4   005.PNG      6"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "8df1291e",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv('submission/submission1.csv',index=False)  \n",
    "#index False 꼭 넣기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7c2602a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv('submission/submission1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "cca831c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 199 entries, 0 to 198\n",
      "Data columns (total 2 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   file_name  199 non-null    object\n",
      " 1   label      199 non-null    int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 3.2+ KB\n"
     ]
    }
   ],
   "source": [
    "submission.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d6474922",
   "metadata": {},
   "outputs": [],
   "source": [
    "samplesubmission = pd.read_csv('dataset/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "bd656707",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 199 entries, 0 to 198\n",
      "Data columns (total 2 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   file_name  199 non-null    object\n",
      " 1   label      199 non-null    int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 3.2+ KB\n"
     ]
    }
   ],
   "source": [
    "samplesubmission.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b70817d8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
